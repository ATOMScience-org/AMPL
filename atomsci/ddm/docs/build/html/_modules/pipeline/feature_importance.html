<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>pipeline.feature_importance &mdash; ATOM Data-Driven Modeling Pipeline 1.5.0 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            ATOM Data-Driven Modeling Pipeline
          </a>
              <div class="version">
                1.5.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../guide/getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guide/install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guide/tests.html">Tests</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guide/running_ampl.html">Running AMPL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guide/advanced_ampl_usage.html">Advanced AMPL Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guide/advanced_installation.html">Advanced Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guide/advanced_testing.html">Advanced Testing</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../modules.html">atomsci</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">ATOM Data-Driven Modeling Pipeline</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Module code</a></li>
      <li class="breadcrumb-item active">pipeline.feature_importance</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for pipeline.feature_importance</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Functions to assess feature importance in AMPL models</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">pdb</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>


<span class="kn">from</span> <span class="nn">atomsci.ddm.pipeline</span> <span class="kn">import</span> <span class="n">model_pipeline</span> <span class="k">as</span> <span class="n">mp</span>
<span class="kn">from</span> <span class="nn">atomsci.ddm.pipeline</span> <span class="kn">import</span> <span class="n">model_datasets</span>
<span class="kn">from</span> <span class="nn">atomsci.ddm.pipeline</span> <span class="kn">import</span> <span class="n">compare_models</span> <span class="k">as</span> <span class="n">cmp</span>
<span class="kn">from</span> <span class="nn">atomsci.ddm.pipeline</span> <span class="kn">import</span> <span class="n">parameter_parser</span> <span class="k">as</span> <span class="n">parse</span>
<span class="kn">from</span> <span class="nn">atomsci.ddm.utils</span> <span class="kn">import</span> <span class="n">datastore_functions</span> <span class="k">as</span> <span class="n">dsf</span>
<span class="kn">from</span> <span class="nn">atomsci.ddm.pipeline.perf_data</span> <span class="kn">import</span> <span class="n">negative_predictive_value</span>

<span class="kn">from</span> <span class="nn">deepchem.data.datasets</span> <span class="kn">import</span> <span class="n">NumpyDataset</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">spearmanr</span>
<span class="kn">from</span> <span class="nn">scipy.cluster</span> <span class="kn">import</span> <span class="n">hierarchy</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># The following import requires scikit-learn &gt;= 0.23.1</span>
<span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">permutation_importance</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)-15s</span><span class="s1"> </span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_SklearnRegressorWrapper</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class that implements the parts of the scikit-learn Estimator interface needed by the</span>
<span class="sd">    permutation importance code for AMPL regression models.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_pipeline</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">model_pipeline</span><span class="o">.</span><span class="n">params</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model_pipeline</span><span class="o">.</span><span class="n">model_wrapper</span><span class="o">.</span><span class="n">model</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">NumpyDataset</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="k">class</span> <span class="nc">_SklearnClassifierWrapper</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class that implements the parts of the scikit-learn Estimator interface needed by the</span>
<span class="sd">    permutation importance code for AMPL classification models.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_pipeline</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">model_pipeline</span><span class="o">.</span><span class="n">params</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model_pipeline</span><span class="o">.</span><span class="n">model_wrapper</span><span class="o">.</span><span class="n">model</span>
        <span class="c1"># TODO: Change for &gt; 2 classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">NumpyDataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># change to return class labels</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">NumpyDataset</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">NumpyDataset</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">probs</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>



<span class="k">def</span> <span class="nf">_get_estimator</span><span class="p">(</span><span class="n">model_pipeline</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given an AMPL ModelPipeline object, returns an object that supports the scikit-learn estimator interface (in particular, </span>
<span class="sd">    the predict and predict_proba methods), for the purpose of running the permutation_importance function.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_pipeline (ModelPipeline): AMPL model pipeline for a trained model</span>

<span class="sd">    Returns:</span>
<span class="sd">        estimator (sklearn.base.BaseEstimator): A scikit-learn Estimator object for the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pparams</span> <span class="o">=</span> <span class="n">model_pipeline</span><span class="o">.</span><span class="n">params</span>
    <span class="n">wrapper</span> <span class="o">=</span> <span class="n">model_pipeline</span><span class="o">.</span><span class="n">model_wrapper</span>
    <span class="k">if</span> <span class="n">pparams</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;RF&#39;</span><span class="p">:</span>
        <span class="c1"># DeepChem model is a wrapper for an sklearn model, so return that</span>
        <span class="k">return</span> <span class="n">wrapper</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span>
    <span class="k">elif</span> <span class="n">pparams</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;xgboost&#39;</span><span class="p">:</span>
        <span class="c1"># XGBoost model is wrapped by an sklearn model</span>
        <span class="k">return</span> <span class="n">wrapper</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span>
    <span class="k">elif</span> <span class="n">pparams</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;hybrid&#39;</span><span class="p">:</span>
        <span class="c1"># TODO: Hybrid model requires special handling because of the two types of predictions</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Hybrid models not supported yet&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">pparams</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;NN&#39;</span><span class="p">:</span>
        <span class="c1"># TODO: Find out if this branch will work for new DeepChem/PyTorch models (AttentiveFP, MPNN, etc.)</span>
        <span class="k">if</span> <span class="n">pparams</span><span class="o">.</span><span class="n">prediction_type</span> <span class="o">==</span> <span class="s1">&#39;regression&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_SklearnRegressorWrapper</span><span class="p">(</span><span class="n">model_pipeline</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_SklearnClassifierWrapper</span><span class="p">(</span><span class="n">model_pipeline</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported model type </span><span class="si">{</span><span class="n">pparams</span><span class="o">.</span><span class="n">model_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_get_scorer</span><span class="p">(</span><span class="n">score_type</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns an sklearn.metrics.Scorer object that can be used to get model performance scores for</span>
<span class="sd">    various input feature sets.</span>

<span class="sd">    Args:</span>
<span class="sd">        score_type (str): Name of the scoring metric to use. This can be any of the standard values supported</span>
<span class="sd">        by sklearn.metrics.get_scorer; the AMPL-specific values &#39;npv&#39;, &#39;mcc&#39;, &#39;kappa&#39;, &#39;mae&#39;, &#39;rmse&#39;, &#39;ppv&#39;,</span>
<span class="sd">        &#39;cross_entropy&#39;, &#39;bal_accuracy&#39; and &#39;avg_precision&#39; are also supported. Score types for which smaller</span>
<span class="sd">        values are better, such as &#39;mae&#39;, &#39;rmse&#39; and &#39;cross_entropy&#39; are mapped to their negative counterparts.</span>

<span class="sd">    Returns:</span>
<span class="sd">        scorer (callable): Function to compute scores for the given metric, such that greater scores are always better.</span>
<span class="sd">        This will have the signature `(estimator, X, y)`, where `estimator` is a model, `X` is the feature array and `y`</span>
<span class="sd">        is an array of ground truth labels.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Handle the cases where the metric isn&#39;t implemented in scikit-learn, or is but doesn&#39;t have a predefined</span>
    <span class="c1"># label recognized by metrics.get_scorer</span>
    <span class="k">if</span> <span class="n">score_type</span> <span class="o">==</span> <span class="s1">&#39;npv&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">metrics</span><span class="o">.</span><span class="n">make_scorer</span><span class="p">(</span><span class="n">negative_predictive_value</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">score_type</span> <span class="o">==</span> <span class="s1">&#39;mcc&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">metrics</span><span class="o">.</span><span class="n">make_scorer</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">matthews_corrcoef</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">score_type</span> <span class="o">==</span> <span class="s1">&#39;kappa&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">metrics</span><span class="o">.</span><span class="n">make_scorer</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">cohen_kappa_score</span><span class="p">)</span>

    <span class="c1"># Otherwise, map the score types used in AMPL to the ones used in scikit-learn in the cases where they are different</span>
    <span class="n">score_type_map</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">mae</span> <span class="o">=</span> <span class="s1">&#39;neg_mean_absolute_error&#39;</span><span class="p">,</span>
            <span class="n">rmse</span> <span class="o">=</span> <span class="s1">&#39;neg_root_mean_squared_error&#39;</span><span class="p">,</span>
            <span class="n">ppv</span> <span class="o">=</span> <span class="s1">&#39;precision&#39;</span><span class="p">,</span>
            <span class="n">cross_entropy</span> <span class="o">=</span> <span class="s1">&#39;neg_log_loss&#39;</span><span class="p">,</span>
            <span class="n">bal_accuracy</span> <span class="o">=</span> <span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span>
            <span class="n">avg_precision</span> <span class="o">=</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">)</span>
    <span class="n">sklearn_score_type</span> <span class="o">=</span> <span class="n">score_type_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">score_type</span><span class="p">,</span> <span class="n">score_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">metrics</span><span class="o">.</span><span class="n">get_scorer</span><span class="p">(</span><span class="n">sklearn_score_type</span><span class="p">)</span>


<span class="c1"># ===================================================================================================</span>
<div class="viewcode-block" id="base_feature_importance"><a class="viewcode-back" href="../../pipeline.html#pipeline.feature_importance.base_feature_importance">[docs]</a><span class="k">def</span> <span class="nf">base_feature_importance</span><span class="p">(</span><span class="n">model_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Minimal baseline feature importance function. Given an AMPL model (or the parameters to train a model),</span>
<span class="sd">    returns a data frame with a row for each feature. The columns of the data frame depend on the model type and</span>
<span class="sd">    prediction type. If the model is a binary classifier, the columns include  t-statistics and p-values</span>
<span class="sd">    for the differences between the means of the active and inactive compounds. If the model is a random forest,</span>
<span class="sd">    the columns will include the mean decrease in impurity (MDI) of each feature, computed by the scikit-learn</span>
<span class="sd">    feature_importances_ function. See the scikit-learn documentation for warnings about interpreting the MDI</span>
<span class="sd">    importance. For all models, the returned data frame will include feature names, means and standard deviations</span>
<span class="sd">    for each feature.</span>

<span class="sd">    This function has been tested on RFs and NNs with rdkit descriptors. Other models and feature combinations</span>
<span class="sd">    may not be supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_pipeline (`ModelPipeline`): A pipeline object for a model that was trained in the current Python session</span>
<span class="sd">        or loaded from the model tracker or a tarball file. Either model_pipeline or params must be provided.</span>

<span class="sd">        params (`dict`): Parameter dictionary for a model to be trained and analyzed. Either model_pipeline or a</span>
<span class="sd">        params argument must be passed; if both are passed, params is ignored and the parameters from model_pipeline</span>
<span class="sd">        are used.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (imp_df, model_pipeline, pparams) (tuple):</span>
<span class="sd">            imp_df (`DataFrame`): Table of feature importance metrics.</span>
<span class="sd">            model_pipeline (`ModelPipeline`): Pipeline object for model that was passed to or trained by function.</span>
<span class="sd">            pparams (`Namespace`): Parsed parameters of model.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">log</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s1">&#39;ATOM&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">model_pipeline</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Either model_pipeline or params can be None but not both&quot;</span><span class="p">)</span>
        <span class="c1"># Train a model based on the parameters given</span>
        <span class="n">pparams</span> <span class="o">=</span> <span class="n">parse</span><span class="o">.</span><span class="n">wrapper</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="n">model_pipeline</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">ModelPipeline</span><span class="p">(</span><span class="n">pparams</span><span class="p">)</span>
        <span class="n">model_pipeline</span><span class="o">.</span><span class="n">train_model</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;model_pipeline and params were both passed; ignoring params argument and using params from model&quot;</span><span class="p">)</span>
        <span class="n">pparams</span> <span class="o">=</span> <span class="n">model_pipeline</span><span class="o">.</span><span class="n">params</span>

    <span class="c1"># Load the original training, validation and test data, if necessary</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">model_data</span> <span class="o">=</span> <span class="n">model_pipeline</span><span class="o">.</span><span class="n">data</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="n">model_pipeline</span><span class="o">.</span><span class="n">featurization</span> <span class="o">=</span> <span class="n">model_pipeline</span><span class="o">.</span><span class="n">model_wrapper</span><span class="o">.</span><span class="n">featurization</span>
        <span class="n">model_pipeline</span><span class="o">.</span><span class="n">load_featurize_data</span><span class="p">()</span>
        <span class="n">model_data</span> <span class="o">=</span> <span class="n">model_pipeline</span><span class="o">.</span><span class="n">data</span>

    <span class="c1"># Get the list of feature column names</span>
    <span class="c1">#features = model_pipeline.model_wrapper.featurization.get_feature_columns()</span>
    <span class="n">features</span> <span class="o">=</span> <span class="n">model_pipeline</span><span class="o">.</span><span class="n">featurization</span><span class="o">.</span><span class="n">get_feature_columns</span><span class="p">()</span>
    <span class="n">nfeat</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
    <span class="n">imp_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;feature&#39;</span><span class="p">:</span> <span class="n">features</span><span class="p">})</span>

    <span class="c1"># Get the training, validation and test sets (we assume we&#39;re not using K-fold CV). These are DeepChem Dataset objects.</span>
    <span class="p">(</span><span class="n">train_dset</span><span class="p">,</span> <span class="n">valid_dset</span><span class="p">)</span> <span class="o">=</span> <span class="n">model_data</span><span class="o">.</span><span class="n">train_valid_dsets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">test_dset</span> <span class="o">=</span> <span class="n">model_data</span><span class="o">.</span><span class="n">test_dset</span>

    <span class="n">imp_df</span><span class="p">[</span><span class="s1">&#39;mean_value&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_dset</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">imp_df</span><span class="p">[</span><span class="s1">&#39;std_value&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_dset</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pparams</span><span class="o">.</span><span class="n">prediction_type</span> <span class="o">==</span> <span class="s1">&#39;classification&#39;</span><span class="p">:</span>
        <span class="c1"># Compute a t-statistic for each feature for the difference between its mean values for active and inactive compounds</span>
        <span class="n">tstats</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pvalues</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">active</span> <span class="o">=</span> <span class="n">train_dset</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">train_dset</span><span class="o">.</span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">inactive</span> <span class="o">=</span> <span class="n">train_dset</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">train_dset</span><span class="o">.</span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>

        <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Computing t-statistics&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">ifeat</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nfeat</span><span class="p">):</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">ttest_ind</span><span class="p">(</span><span class="n">active</span><span class="p">[:,</span><span class="n">ifeat</span><span class="p">],</span> <span class="n">inactive</span><span class="p">[:,</span><span class="n">ifeat</span><span class="p">],</span> <span class="n">equal_var</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">nan_policy</span><span class="o">=</span><span class="s1">&#39;omit&#39;</span><span class="p">)</span>
            <span class="n">tstats</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">statistic</span><span class="p">)</span>
            <span class="n">pvalues</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">pvalue</span><span class="p">)</span>
        <span class="n">imp_df</span><span class="p">[</span><span class="s1">&#39;t_statistic&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tstats</span>
        <span class="n">imp_df</span><span class="p">[</span><span class="s1">&#39;ttest_pvalue&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pvalues</span>

    <span class="k">if</span> <span class="n">pparams</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;RF&#39;</span><span class="p">:</span>
        <span class="c1"># Tabulate the MDI-based feature importances for random forest models</span>
        <span class="c1"># TODO: Does this work for XGBoost models too?</span>
        <span class="n">rf_model</span> <span class="o">=</span> <span class="n">model_pipeline</span><span class="o">.</span><span class="n">model_wrapper</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model</span>
        <span class="n">imp_df</span><span class="p">[</span><span class="s1">&#39;mdi_importance&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">rf_model</span><span class="o">.</span><span class="n">feature_importances_</span>

    <span class="k">return</span> <span class="n">imp_df</span><span class="p">,</span> <span class="n">model_pipeline</span><span class="p">,</span> <span class="n">pparams</span></div>

<span class="c1"># ===================================================================================================</span>
<div class="viewcode-block" id="permutation_feature_importance"><a class="viewcode-back" href="../../pipeline.html#pipeline.feature_importance.permutation_feature_importance">[docs]</a><span class="k">def</span> <span class="nf">permutation_feature_importance</span><span class="p">(</span><span class="n">model_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">score_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nreps</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">nworkers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                   <span class="n">result_file</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Assess the importance of each feature used by a trained model by permuting the values of each feature in succession</span>
<span class="sd">    in the training, validation and test sets, making predictions, computing performance metrics, and measuring the effect</span>
<span class="sd">    of scrambling each feature on a particular metric.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_pipeline (`ModelPipeline`): A pipeline object for a model that was trained in the current Python session</span>
<span class="sd">        or loaded from the model tracker or a tarball file. Either `model_pipeline` or `params` must be provided.</span>

<span class="sd">        params (`dict`): Parameter dictionary for a model to be trained and analyzed. Either `model_pipeline` or a</span>
<span class="sd">        `params` argument must be passed; if both are passed, `params` is ignored and the parameters from `model_pipeline`</span>
<span class="sd">        are used.</span>

<span class="sd">        score_type (str): Name of the scoring metric to use to assess importance. This can be any of the standard values</span>
<span class="sd">        supported by sklearn.metrics.get_scorer; the AMPL-specific values &#39;npv&#39;, &#39;mcc&#39;, &#39;kappa&#39;, &#39;mae&#39;, &#39;rmse&#39;, &#39;ppv&#39;,</span>
<span class="sd">        &#39;cross_entropy&#39;, &#39;bal_accuracy&#39; and &#39;avg_precision&#39; are also supported. Score types for which smaller</span>
<span class="sd">        values are better, such as &#39;mae&#39;, &#39;rmse&#39; and &#39;cross_entropy&#39; are mapped to their negative counterparts.</span>

<span class="sd">        nreps (int): Number of repetitions of the permutation and rescoring procedure to perform for each feature; the </span>
<span class="sd">        importance values returned will be averages over repetitions. More repetitions will yield better importance</span>
<span class="sd">        estimates at the cost of greater computing time.</span>

<span class="sd">        nworkers (int): Number of parallel worker threads to use for permutation and rescoring.</span>

<span class="sd">        result_file (str): Optional path to a CSV file to which the importance table will be written.</span>

<span class="sd">    Returns:</span>
<span class="sd">        imp_df (DataFrame): Table of features and importance metrics. The table will include the columns returned by</span>
<span class="sd">        `base_feature_importance`, along with the permutation importance scores for each feature for the training, validation</span>
<span class="sd">        and test subsets.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">log</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s1">&#39;ATOM&#39;</span><span class="p">)</span>
    <span class="n">imp_df</span><span class="p">,</span> <span class="n">model_pipeline</span><span class="p">,</span> <span class="n">pparams</span> <span class="o">=</span> <span class="n">base_feature_importance</span><span class="p">(</span><span class="n">model_pipeline</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="c1"># Compute the permutation-based importance values for the training, validation and test sets</span>
    <span class="n">estimator</span> <span class="o">=</span> <span class="n">_get_estimator</span><span class="p">(</span><span class="n">model_pipeline</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">score_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">score_type</span> <span class="o">=</span> <span class="n">pparams</span><span class="o">.</span><span class="n">model_choice_score_type</span>
    <span class="n">scorer</span> <span class="o">=</span> <span class="n">_get_scorer</span><span class="p">(</span><span class="n">score_type</span><span class="p">)</span>

    <span class="c1"># Get the training, validation and test sets (we assume we&#39;re not using K-fold CV). These are DeepChem Dataset objects.</span>
    <span class="p">(</span><span class="n">train_dset</span><span class="p">,</span> <span class="n">valid_dset</span><span class="p">)</span> <span class="o">=</span> <span class="n">model_pipeline</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">train_valid_dsets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">test_dset</span> <span class="o">=</span> <span class="n">model_pipeline</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">test_dset</span>
    <span class="n">subsets</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">train</span><span class="o">=</span><span class="n">train_dset</span><span class="p">,</span> <span class="n">valid</span><span class="o">=</span><span class="n">valid_dset</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="n">test_dset</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">subset</span><span class="p">,</span> <span class="n">dset</span> <span class="ow">in</span> <span class="n">subsets</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Computing permutation importance for </span><span class="si">{</span><span class="n">subset</span><span class="si">}</span><span class="s2"> set...&quot;</span><span class="p">)</span>
        <span class="n">pi_result</span> <span class="o">=</span> <span class="n">permutation_importance</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">dset</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">dset</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scorer</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="n">nreps</span><span class="p">,</span> 
            <span class="n">random_state</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">nworkers</span><span class="p">)</span>
        <span class="n">imp_df</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">subset</span><span class="si">}</span><span class="s2">_perm_importance_mean&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pi_result</span><span class="p">[</span><span class="s1">&#39;importances_mean&#39;</span><span class="p">]</span>
        <span class="n">imp_df</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">subset</span><span class="si">}</span><span class="s2">_perm_importance_std&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pi_result</span><span class="p">[</span><span class="s1">&#39;importances_std&#39;</span><span class="p">]</span>
    <span class="n">imp_df</span> <span class="o">=</span> <span class="n">imp_df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;valid_perm_importance_mean&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">result_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">imp_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">result_file</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Wrote importance table to </span><span class="si">{</span><span class="n">result_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">imp_df</span></div>

<span class="c1"># ===================================================================================================</span>
<div class="viewcode-block" id="plot_feature_importances"><a class="viewcode-back" href="../../pipeline.html#pipeline.feature_importance.plot_feature_importances">[docs]</a><span class="k">def</span> <span class="nf">plot_feature_importances</span><span class="p">(</span><span class="n">imp_df</span><span class="p">,</span> <span class="n">importance_col</span><span class="o">=</span><span class="s1">&#39;valid_perm_importance_mean&#39;</span><span class="p">,</span> <span class="n">max_feat</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Display a horizontal bar plot showing the relative importances of the most important features or feature clusters, according to</span>
<span class="sd">    the results of `permutation_feature_importance`, `cluster_permutation_importance` or a similar function.</span>

<span class="sd">    Args:</span>
<span class="sd">        imp_df (DataFrame): Table of results from `permutation_feature_importance`, `cluster_permutation_importance`,</span>
<span class="sd">        `base_feature_importance` or a similar function.</span>

<span class="sd">        importance_col (str): Name of the column in `imp_df` to plot values from.</span>

<span class="sd">        max_feat (int): The maximum number of features or feature clusters to plot values for.</span>

<span class="sd">        ascending (bool): Should the features be ordered by ascending values of `importance_col`? Defaults to False; can be set True</span>
<span class="sd">        for p-values or something else where small values mean greater importance.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>
    <span class="n">fi_df</span> <span class="o">=</span> <span class="n">imp_df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="n">importance_col</span><span class="p">,</span>  <span class="n">ascending</span><span class="o">=</span><span class="n">ascending</span><span class="p">)</span>
    <span class="k">if</span> <span class="s1">&#39;cluster_id&#39;</span> <span class="ow">in</span> <span class="n">fi_df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">tolist</span><span class="p">():</span>
        <span class="n">feat_col</span> <span class="o">=</span> <span class="s1">&#39;features&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">feat_col</span> <span class="o">=</span> <span class="s1">&#39;feature&#39;</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">importance_col</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">feat_col</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">fi_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">max_feat</span><span class="p">))</span></div>

<span class="c1"># ===================================================================================================</span>
<div class="viewcode-block" id="display_feature_clusters"><a class="viewcode-back" href="../../pipeline.html#pipeline.feature_importance.display_feature_clusters">[docs]</a><span class="k">def</span> <span class="nf">display_feature_clusters</span><span class="p">(</span><span class="n">model_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">clust_height</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                   <span class="n">corr_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_matrix</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_dendro</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Cluster the input features used in the model specified by `model_pipeline` or `params`, using Spearman correlation</span>
<span class="sd">    as a similarity metric. Display a dendrogram and/or a correlation matrix heatmap, so the user can decide the</span>
<span class="sd">    height at which to cut the dendrogram in order to split the features into clusters, for input to</span>
<span class="sd">    `cluster_permutation_importance`.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_pipeline (`ModelPipeline`): A pipeline object for a model that was trained in the current Python session</span>
<span class="sd">        or loaded from the model tracker or a tarball file. Either `model_pipeline` or `params` must be provided.</span>

<span class="sd">        params (`dict`): Parameter dictionary for a model to be trained and analyzed. Either `model_pipeline` or a</span>
<span class="sd">        `params` argument must be passed; if both are passed, `params` is ignored and the parameters from `model_pipeline`</span>
<span class="sd">        are used.</span>

<span class="sd">        clust_height (float): Height at which to draw a cut line in the dendrogram, to show how many clusters</span>
<span class="sd">        will be generated.</span>

<span class="sd">        corr_file (str): Path to an optional CSV file to be created containing the feature correlation matrix.</span>

<span class="sd">        show_matrix (bool): If True, plot a correlation matrix heatmap.</span>

<span class="sd">        show_dendro (bool): If True, plot the dendrogram.</span>

<span class="sd">    Returns:</span>
<span class="sd">        corr_linkage (np.ndarray): Linkage matrix from correlation clustering</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">log</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s1">&#39;ATOM&#39;</span><span class="p">)</span>
    <span class="n">imp_df</span><span class="p">,</span> <span class="n">model_pipeline</span><span class="p">,</span> <span class="n">pparams</span> <span class="o">=</span> <span class="n">base_feature_importance</span><span class="p">(</span><span class="n">model_pipeline</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="n">features</span> <span class="o">=</span> <span class="n">imp_df</span><span class="o">.</span><span class="n">feature</span><span class="o">.</span><span class="n">values</span>

    <span class="c1"># Get the training, validation and test sets (we assume we&#39;re not using K-fold CV). These are DeepChem Dataset objects.</span>
    <span class="p">(</span><span class="n">train_dset</span><span class="p">,</span> <span class="n">valid_dset</span><span class="p">)</span> <span class="o">=</span> <span class="n">model_pipeline</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">train_valid_dsets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Eliminate features that don&#39;t vary over the training set (and thus have zero importance)</span>
    <span class="n">feat_idx</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">feat</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">train_dset</span><span class="o">.</span><span class="n">X</span><span class="p">[:,</span><span class="n">i</span><span class="p">]))</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">feat_idx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Removed unvarying feature </span><span class="si">{</span><span class="n">feat</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">feat_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">feat_idx</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">clust_X</span> <span class="o">=</span> <span class="n">train_dset</span><span class="o">.</span><span class="n">X</span><span class="p">[:,</span><span class="n">feat_idx</span><span class="p">]</span>
    <span class="n">imp_df</span> <span class="o">=</span> <span class="n">imp_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">feat_idx</span><span class="p">]</span>
    <span class="n">var_features</span> <span class="o">=</span> <span class="n">imp_df</span><span class="o">.</span><span class="n">feature</span><span class="o">.</span><span class="n">values</span>

    <span class="c1"># Cluster the training set features</span>
    <span class="n">corr</span> <span class="o">=</span> <span class="n">spearmanr</span><span class="p">(</span><span class="n">clust_X</span><span class="p">,</span> <span class="n">nan_policy</span><span class="o">=</span><span class="s1">&#39;omit&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">correlation</span>
    <span class="n">corr_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="n">var_features</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">feat</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">var_features</span><span class="p">):</span>
        <span class="n">corr_df</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">corr</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">corr_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">corr_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">corr_file</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Wrote correlation matrix to </span><span class="si">{</span><span class="n">corr_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">corr_linkage</span> <span class="o">=</span> <span class="n">hierarchy</span><span class="o">.</span><span class="n">ward</span><span class="p">(</span><span class="n">corr</span><span class="p">)</span>
    <span class="n">cluster_ids</span> <span class="o">=</span> <span class="n">hierarchy</span><span class="o">.</span><span class="n">fcluster</span><span class="p">(</span><span class="n">corr_linkage</span><span class="p">,</span> <span class="n">clust_height</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;distance&#39;</span><span class="p">)</span>
    <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cutting dendrogram at height </span><span class="si">{</span><span class="n">clust_height</span><span class="si">}</span><span class="s2"> yields </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">cluster_ids</span><span class="p">))</span><span class="si">}</span><span class="s2"> clusters&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">show_dendro</span><span class="p">:</span>
        <span class="n">dendro</span> <span class="o">=</span> <span class="n">hierarchy</span><span class="o">.</span><span class="n">dendrogram</span><span class="p">(</span><span class="n">corr_linkage</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">var_features</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">no_plot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">leaf_rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
        <span class="n">dendro</span> <span class="o">=</span> <span class="n">hierarchy</span><span class="o">.</span><span class="n">dendrogram</span><span class="p">(</span><span class="n">corr_linkage</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">var_features</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">leaf_rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
        <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="c1"># Plot horizontal dashed line at clust_height</span>
        <span class="n">line</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">clust_height</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">show_matrix</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span><span class="mi">25</span><span class="p">))</span>
        <span class="n">dendro_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dendro</span><span class="p">[</span><span class="s1">&#39;ivl&#39;</span><span class="p">]))</span>
        <span class="n">leaves</span> <span class="o">=</span> <span class="n">dendro</span><span class="p">[</span><span class="s1">&#39;leaves&#39;</span><span class="p">]</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">corr</span><span class="p">[</span><span class="n">leaves</span><span class="p">,</span> <span class="p">:][:,</span> <span class="n">leaves</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">dendro_idx</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">dendro_idx</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">dendro</span><span class="p">[</span><span class="s1">&#39;ivl&#39;</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="s1">&#39;vertical&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">dendro</span><span class="p">[</span><span class="s1">&#39;ivl&#39;</span><span class="p">])</span>
        <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">corr_linkage</span></div>


<span class="c1"># ===================================================================================================</span>
<div class="viewcode-block" id="cluster_permutation_importance"><a class="viewcode-back" href="../../pipeline.html#pipeline.feature_importance.cluster_permutation_importance">[docs]</a><span class="k">def</span> <span class="nf">cluster_permutation_importance</span><span class="p">(</span><span class="n">model_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">score_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">clust_height</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                   <span class="n">result_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nreps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">nworkers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divide the input features used in a model into correlated clusters, then assess the importance of the features</span>
<span class="sd">    by iterating over clusters, permuting the values of all the features in the cluster, and measuring the effect</span>
<span class="sd">    on the model performance metric given by score_type for the training, validation and test subsets.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_pipeline (`ModelPipeline`): A pipeline object for a model that was trained in the current Python session</span>
<span class="sd">        or loaded from the model tracker or a tarball file. Either `model_pipeline` or `params` must be provided.</span>

<span class="sd">        params (`dict`): Parameter dictionary for a model to be trained and analyzed. Either `model_pipeline` or a</span>
<span class="sd">        `params` argument must be passed; if both are passed, `params` is ignored and the parameters from `model_pipeline`</span>
<span class="sd">        are used.</span>

<span class="sd">        clust_height (float): Height at which to cut the dendrogram branches to split features into clusters.</span>

<span class="sd">        result_file (str): Path to a CSV file where a table of features and cluster indices will be written.</span>

<span class="sd">        nreps (int): Number of repetitions of the permutation and rescoring procedure to perform for each feature; the </span>
<span class="sd">        importance values returned will be averages over repetitions. More repetitions will yield better importance</span>
<span class="sd">        estimates at the cost of greater computing time.</span>

<span class="sd">        nworkers (int): Number of parallel worker threads to use for permutation and rescoring. Currently ignored; multithreading</span>
<span class="sd">        will be added in a future version.</span>

<span class="sd">    Returns:</span>
<span class="sd">        imp_df (DataFrame): Table of feature clusters and importance values</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">log</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s1">&#39;ATOM&#39;</span><span class="p">)</span>
    <span class="n">imp_df</span><span class="p">,</span> <span class="n">model_pipeline</span><span class="p">,</span> <span class="n">pparams</span> <span class="o">=</span> <span class="n">base_feature_importance</span><span class="p">(</span><span class="n">model_pipeline</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="n">features</span> <span class="o">=</span> <span class="n">imp_df</span><span class="o">.</span><span class="n">feature</span><span class="o">.</span><span class="n">values</span>

    <span class="c1"># Compute the permutation-based importance values for the training, validation and test sets</span>
    <span class="n">estimator</span> <span class="o">=</span> <span class="n">_get_estimator</span><span class="p">(</span><span class="n">model_pipeline</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">score_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">score_type</span> <span class="o">=</span> <span class="n">pparams</span><span class="o">.</span><span class="n">model_choice_score_type</span>
    <span class="n">scorer</span> <span class="o">=</span> <span class="n">_get_scorer</span><span class="p">(</span><span class="n">score_type</span><span class="p">)</span>

    <span class="c1"># Get the training, validation and test sets (we assume we&#39;re not using K-fold CV). These are DeepChem Dataset objects.</span>
    <span class="p">(</span><span class="n">train_dset</span><span class="p">,</span> <span class="n">valid_dset</span><span class="p">)</span> <span class="o">=</span> <span class="n">model_pipeline</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">train_valid_dsets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">test_dset</span> <span class="o">=</span> <span class="n">model_pipeline</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">test_dset</span>

    <span class="c1"># Eliminate features that don&#39;t vary over the training set (and thus have zero importance)</span>
    <span class="n">feat_idx</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">feat</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">train_dset</span><span class="o">.</span><span class="n">X</span><span class="p">[:,</span><span class="n">i</span><span class="p">]))</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">feat_idx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">feat_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">feat_idx</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">clust_X</span> <span class="o">=</span> <span class="n">train_dset</span><span class="o">.</span><span class="n">X</span><span class="p">[:,</span><span class="n">feat_idx</span><span class="p">]</span>
    <span class="n">imp_df</span> <span class="o">=</span> <span class="n">imp_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">feat_idx</span><span class="p">]</span>
    <span class="n">var_features</span> <span class="o">=</span> <span class="n">imp_df</span><span class="o">.</span><span class="n">feature</span><span class="o">.</span><span class="n">values</span>

    <span class="c1"># Cluster the training set features</span>
    <span class="n">corr</span> <span class="o">=</span> <span class="n">spearmanr</span><span class="p">(</span><span class="n">clust_X</span><span class="p">,</span> <span class="n">nan_policy</span><span class="o">=</span><span class="s1">&#39;omit&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">correlation</span>
    <span class="n">corr_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="n">var_features</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">feat</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">var_features</span><span class="p">):</span>
        <span class="n">corr_df</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span> <span class="o">=</span> <span class="n">corr</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span>
    <span class="n">corr_linkage</span> <span class="o">=</span> <span class="n">hierarchy</span><span class="o">.</span><span class="n">ward</span><span class="p">(</span><span class="n">corr</span><span class="p">)</span>

    <span class="n">cluster_ids</span> <span class="o">=</span> <span class="n">hierarchy</span><span class="o">.</span><span class="n">fcluster</span><span class="p">(</span><span class="n">corr_linkage</span><span class="p">,</span> <span class="n">clust_height</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;distance&#39;</span><span class="p">)</span>
    <span class="n">clust_to_feat_ids</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="n">clust_to_feat_names</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cluster_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cluster_ids</span><span class="p">):</span>
        <span class="c1"># clust_to_feat_ids will contain indices in original feature list</span>
        <span class="n">clust_to_feat_ids</span><span class="p">[</span><span class="n">cluster_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">feat_idx</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">clust_to_feat_names</span><span class="p">[</span><span class="n">cluster_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">var_features</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">clust_idx</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">clust_to_feat_ids</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
    <span class="n">clust_sizes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">clust_to_feat_ids</span><span class="p">[</span><span class="n">clust</span><span class="p">])</span> <span class="k">for</span> <span class="n">clust</span> <span class="ow">in</span> <span class="n">clust_idx</span><span class="p">])</span>
    <span class="n">clust_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">clust_to_feat_names</span><span class="p">[</span><span class="n">clust</span><span class="p">])</span> <span class="k">for</span> <span class="n">clust</span> <span class="ow">in</span> <span class="n">clust_idx</span><span class="p">]</span>
    <span class="n">n_non_sing</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">clust_sizes</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cutting dendrogram at height </span><span class="si">{</span><span class="n">clust_height</span><span class="si">}</span><span class="s2"> yields </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">cluster_ids</span><span class="p">))</span><span class="si">}</span><span class="s2"> clusters&quot;</span><span class="p">)</span>
    <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">n_non_sing</span><span class="si">}</span><span class="s2"> are non-singletons&quot;</span><span class="p">)</span>
    <span class="n">clust_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">cluster_id</span><span class="o">=</span><span class="n">clust_idx</span><span class="p">,</span> <span class="n">num_feat</span><span class="o">=</span><span class="n">clust_sizes</span><span class="p">,</span> <span class="n">features</span><span class="o">=</span><span class="n">clust_labels</span><span class="p">))</span>
    <span class="n">clust_df</span> <span class="o">=</span> <span class="n">clust_df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;num_feat&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Now iterate through clusters; for each cluster, permute all the features in the cluster</span>
    <span class="n">subsets</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">train</span><span class="o">=</span><span class="n">train_dset</span><span class="p">,</span> <span class="n">valid</span><span class="o">=</span><span class="n">valid_dset</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="n">test_dset</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">subset</span><span class="p">,</span> <span class="n">dset</span> <span class="ow">in</span> <span class="n">subsets</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Computing permutation importance for </span><span class="si">{</span><span class="n">subset</span><span class="si">}</span><span class="s2"> set...&quot;</span><span class="p">)</span>
        <span class="c1"># First the score without permuting anything</span>
        <span class="n">baseline_score</span> <span class="o">=</span> <span class="n">scorer</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">dset</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">dset</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>
        <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Baseline </span><span class="si">{</span><span class="n">subset</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">score_type</span><span class="si">}</span><span class="s2"> score = </span><span class="si">{</span><span class="n">baseline_score</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">random_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">17</span><span class="p">)</span>
        <span class="c1">#random_seed = random_state.randint(np.iinfo(np.int32).max + 1)</span>
        <span class="n">importances_mean</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">importances_std</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">clust</span> <span class="ow">in</span> <span class="n">clust_df</span><span class="o">.</span><span class="n">cluster_id</span><span class="o">.</span><span class="n">values</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">_calc_cluster_permutation_scores</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">dset</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">dset</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">clust_to_feat_ids</span><span class="p">[</span><span class="n">clust</span><span class="p">],</span>
                                                      <span class="n">random_state</span><span class="p">,</span> <span class="n">nreps</span><span class="p">,</span> <span class="n">scorer</span><span class="p">)</span>
            <span class="n">importances_mean</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">baseline_score</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span>
            <span class="n">importances_std</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span>

        <span class="n">clust_df</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">subset</span><span class="si">}</span><span class="s2">_perm_importance_mean&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">importances_mean</span>
        <span class="n">clust_df</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">subset</span><span class="si">}</span><span class="s2">_perm_importance_std&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">importances_std</span>

    <span class="n">imp_df</span> <span class="o">=</span> <span class="n">clust_df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;valid_perm_importance_mean&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">result_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">imp_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">result_file</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Wrote cluster importances to </span><span class="si">{</span><span class="n">result_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">imp_df</span></div>

<span class="c1"># ===================================================================================================</span>
<span class="k">def</span> <span class="nf">_calc_cluster_permutation_scores</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">col_indices</span><span class="p">,</span> <span class="n">random_state</span><span class="p">,</span> <span class="n">n_repeats</span><span class="p">,</span> <span class="n">scorer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate score of estimator when `col_indices` are all permuted randomly.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Work on a copy of X to to ensure thread-safety in case of threading based</span>
    <span class="c1"># parallelism. </span>
    <span class="n">X_permuted</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_repeats</span><span class="p">)</span>
    <span class="n">shuffling_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">n_round</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_repeats</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">col_idx</span> <span class="ow">in</span> <span class="n">col_indices</span><span class="p">:</span>
            <span class="n">random_state</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">shuffling_idx</span><span class="p">)</span>
            <span class="n">X_permuted</span><span class="p">[:,</span> <span class="n">col_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_permuted</span><span class="p">[</span><span class="n">shuffling_idx</span><span class="p">,</span> <span class="n">col_idx</span><span class="p">]</span>
        <span class="n">feature_score</span> <span class="o">=</span> <span class="n">scorer</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X_permuted</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">scores</span><span class="p">[</span><span class="n">n_round</span><span class="p">]</span> <span class="o">=</span> <span class="n">feature_score</span>

    <span class="k">return</span> <span class="n">scores</span>



</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, ATOM DDM Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>