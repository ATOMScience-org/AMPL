{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting solubility using the ATOM Modeling Pipeline (AMPL) on the public Delaney solubility dataset\n",
    "\n",
    "In this notebook, we describe the AMPL used to curate a public dataset, fit a simple model to predict solubility from chemical structure, and predict solubility for withheld compounds.\n",
    "\n",
    "## Set up\n",
    "We first import the AMPL modules for use in this notebook.\n",
    "\n",
    "The relevant AMPL modules for this example are listed below:\n",
    "\n",
    "|module|Description|\n",
    "|-|-|\n",
    "|`atomsci.ddm.pipeline.model_pipeline`|The model pipeline module is used to fit models and load models for prediction.|\n",
    "|`atomsci.ddm.pipeline.parameter_parser`|The parameter parser reads through pipeline options for the model pipeline.|\n",
    "|`atomsci.ddm.utils.curate_data`|The curate data module is used for data loading and pre-processing.|\n",
    "|`atomsci.ddm.utils.struct_utils`|The structure utilities module is used to process loaded structures.|\n",
    "|`atomsci.ddm.pipeline.perf_plots`|Perf plots contains a variety of plotting functions.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import sys\n",
    "        \n",
    "# We temporarily disable warnings for demonstration.\n",
    "# FutureWarnings and DeprecationWarnings are present from some of the AMPL dependency modules.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import atomsci.ddm.pipeline.model_pipeline as mp\n",
    "import atomsci.ddm.pipeline.parameter_parser as parse\n",
    "import atomsci.ddm.utils.curate_data as curate_data\n",
    "import atomsci.ddm.utils.struct_utils as struct_utils\n",
    "from atomsci.ddm.pipeline import perf_plots as pp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data curation\n",
    "\n",
    "We then download and do very simple curation to the related dataset.\n",
    "\n",
    "We need to set the directory we want to save files to. Next we download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = '/usr/local/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Delaney dataset\n",
    "dataset_file = os.path.join(working_dir, 'delaney-processed.csv')\n",
    "if (not os.path.isfile(dataset_file)):\n",
    "    r = requests.get('http://deepchem.io.s3-website-us-west-1.amazonaws.com/datasets/delaney-processed.csv', verify=True)\n",
    "    with open(dataset_file, 'wb') as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the downloaded dataset, and process the compound structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Delaney dataset\n",
    "raw_df = pd.read_csv(dataset_file)\n",
    "\n",
    "# Generate SMILES, InChI keys for dataset with curation and structure modules.\n",
    "# RDkit modules are used to process the SMILES strings\n",
    "raw_df['rdkit_smiles'] = raw_df['smiles'].apply(curate_data.base_smiles_from_smiles)\n",
    "raw_df['inchi_key'] = raw_df['smiles'].apply(struct_utils.smiles_to_inchi_key)\n",
    "\n",
    "data = raw_df\n",
    "data['compound_id'] = data['inchi_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to address the case where we have multiple measurements for a single structure (by RDkit canonical SMILEs string). We have a function in the `curate_data()` module to address process compounds. The function parameters are listed below along with an explanation of each parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column: Response values column\n",
    "column = 'measured log solubility in mols per litre'\n",
    "\n",
    "# tolerance: Percentage of individual respsonse values allowed to different from the average to be included in averaging\n",
    "tolerance = 10\n",
    "\n",
    "# list_bad_duplicates: Print structures with bad duplicates\n",
    "list_bad_duplicates = 'Yes'\n",
    "\n",
    "# max_std: Maximum allowed standard deviation for computed average response value\n",
    "# NOTE: In this example, we set this value very high to disable this feature\n",
    "max_std = 100000\n",
    "\n",
    "# compound_id: Compound ID column\n",
    "compound_id = 'compound_id'\n",
    "\n",
    "# smiles_col: SMILES column\n",
    "smiles_col = 'rdkit_smiles'\n",
    "\n",
    "curated_df = curate_data.average_and_remove_duplicates(column, tolerance, list_bad_duplicates, data, max_std,\n",
    "                                                       compound_id=compound_id, smiles_col=smiles_col)\n",
    "curated_file = os.path.join(working_dir, 'delaney_curated.csv')\n",
    "curated_df.to_csv(curated_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a curated dataset, we decide what type of featurizer and model we would like. See documentation for all available options. We also set the name of the new averaged response value column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = 'ecfp'\n",
    "model_type = 'RF'\n",
    "response_cols = ['VALUE_NUM_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set up the parameters for our model. We set datastore and save_results to False to indicate that we are reading the input file and saving the results directly to the file system. There are a wide range of settable parameters; see the documentation for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\"datastore\": \"False\",\n",
    "        \"save_results\": \"False\",\n",
    "        \"id_col\": compound_id,\n",
    "        \"smiles_col\": smiles_col,\n",
    "        \"response_cols\": response_cols,\n",
    "        \"featurizer\": featurizer,\n",
    "        \"model_type\": model_type,\n",
    "        \"result_dir\": working_dir,\n",
    "        \"dataset_key\": curated_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use parse.wrapper to process our input configuration. We then build the model pipeline, train the model, and plot the predicted versus true values for our train, valid, test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pparams = parse.wrapper(params)\n",
    "MP = mp.ModelPipeline(pparams)\n",
    "MP.train_model()\n",
    "pp.plot_pred_vs_actual(MP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
