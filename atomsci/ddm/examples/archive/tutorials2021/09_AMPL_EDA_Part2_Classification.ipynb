{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ATOMScience-org/AMPL/blob/master/atomsci/ddm/examples/tutorials/09_AMPL_EDA_Part2_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFXkxwBu1C4W"
   },
   "source": [
    "# EDA Part two: visualizing results of hyperparameter search and predictions from best models (read-only)\n",
    "\n",
    "**Please note that this template read-only notebook is designed for begineers and provides tips on how to identify good performing models. Our suggestion is use this notebook after you have carried out your HPO search and have access to several models. Please run this notebook on systems with large diskspace and dedicated resources (RAM, CPU)**\n",
    "\n",
    "# Scope of the tutorial:\n",
    "\n",
    "This notebook runs through the same setup as EDA Part 2 (08_AMPL_EDA_Part2.ipynb\n",
    ") but examines classification models instead of regression models. There are some new visualizations for your use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_P9RZAleN14",
    "outputId": "9dfab84b-d744-4196-ea39-26a11073b4d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug  2 19:16:06 UTC 2021\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZ70ierx3pq2"
   },
   "source": [
    "# Install AMPL packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNuv4Q5KFABw"
   },
   "outputs": [],
   "source": [
    "! pip install rdkit-pypi\n",
    "! pip install deepchem\n",
    "\n",
    "import deepchem\n",
    "# print(deepchem.__version__)\n",
    "! pip install umap\n",
    "! pip install -U --ignore-installed numba\n",
    "! pip install umap-learn\n",
    "! pip install molvs\n",
    "! pip install bravado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AB0U-9gyJNB"
   },
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "\n",
    "# get the Install AMPL_GPU_test.sh\n",
    "\n",
    "!wget 'https://raw.githubusercontent.com/ATOMScience-org/AMPL/master/atomsci/ddm/examples/tutorials/config/install_AMPL_GPU_test.sh' >& /dev/null\n",
    "\n",
    "# run the script to install AMPL\n",
    "! chmod u+x install_AMPL_GPU_test.sh\n",
    "! ./install_AMPL_GPU_test.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InUQE4II1noi"
   },
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/ATOMScience-org/AMPL/master/atomsci/ddm/examples/tutorials/datasets/H1_std.csv >& /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTOwqhvR3xL3"
   },
   "source": [
    "# Load packages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeN9kEMO2R9Z"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set_context(\"poster\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "pal = sns.color_palette()\n",
    "\n",
    "import pandas as pd\n",
    "import os, json, sys, glob, pickle\n",
    "\n",
    "from atomsci.ddm.pipeline import model_pipeline as mp\n",
    "from atomsci.ddm.pipeline import parameter_parser as parse\n",
    "from atomsci.ddm.pipeline import perf_data\n",
    "from atomsci.ddm.pipeline import compare_models as cmp\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "coNNIQafgoSC",
    "outputId": "6930e9be-5b41-41ec-f42e-f5fb3bf338c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeCOymX5mvjN"
   },
   "source": [
    "## Please use the following notebook, https://github.com/ATOMScience-org/AMPL/blob/master/atomsci/ddm/examples/tutorials/AMPL_FNL_Workshop_06052021.ipynb, to prepare `HTR3A_curated.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5J-cJNz2V5w"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "\n",
    "h1 = pd.read_csv(\"HTR3A_curated.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_fQuS473N1M"
   },
   "outputs": [],
   "source": [
    "h1=h1[~h1.VALUE_NUM_mean.isna()]\n",
    "h1=h1[h1.VALUE_NUM_mean>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUlF1oDue7Ms"
   },
   "outputs": [],
   "source": [
    "print(len(h1), h1.active.sum())\n",
    "h1.hist('VALUE_NUM_mean');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xePRDvq4X00g"
   },
   "source": [
    "# Edit some AMPL functions to include xgboost parameters\n",
    "Just run this code once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yM0DNyKvX0kZ"
   },
   "outputs": [],
   "source": [
    "nan = np.float32('nan')\n",
    "def get_summary_perf_tables(collection_names=None, filter_dict={}, result_dir=None, prediction_type='regression', verbose=False):\n",
    "    \"\"\"\n",
    "    Load model parameters and performance metrics from model tracker for all models saved in the model tracker DB under\n",
    "    the given collection names (or result directory if Model tracker is not available) with the given prediction type. Tabulate the parameters and metrics including:\n",
    "        dataset (assay name, target, parameter, key, bucket)\n",
    "        dataset size (train/valid/test/total)\n",
    "        number of training folds\n",
    "        model type (NN or RF)\n",
    "        featurizer\n",
    "        transformation type\n",
    "        metrics: r2_score, mae_score and rms_score for regression, or ROC AUC for classification\n",
    "\n",
    "    result_dir: use result_dir when the model tracker is not available. Use a list format if you have multiple result direcotries.\n",
    "    \"\"\"\n",
    "    collection_list = []\n",
    "    model_uuid_list = []\n",
    "    time_built_list = []\n",
    "    prediction_type_list = []\n",
    "    model_type_list = []\n",
    "    dataset_key_list = []\n",
    "    bucket_list = []\n",
    "    param_list = []\n",
    "    featurizer_list = []\n",
    "    desc_type_list = []\n",
    "    transform_list = []\n",
    "    dset_size_list = []\n",
    "    splitter_list = []\n",
    "    split_strategy_list = []\n",
    "    split_uuid_list = []\n",
    "    rf_estimators_list = []\n",
    "    rf_max_features_list = []\n",
    "    rf_max_depth_list = []\n",
    "    best_epoch_list = []\n",
    "    max_epochs_list = []\n",
    "    learning_rate_list = []\n",
    "    layer_sizes_list = []\n",
    "    dropouts_list = []\n",
    "    xgb_gamma_list = []\n",
    "    xgb_learning_rate_list = []\n",
    "    umap_dim_list = []\n",
    "    umap_targ_wt_list = []\n",
    "    umap_neighbors_list = []\n",
    "    umap_min_dist_list = []\n",
    "    split_uuid_list=[]\n",
    "\n",
    "\n",
    "    if prediction_type == 'regression':\n",
    "        score_types = ['r2_score', 'mae_score', 'rms_score']\n",
    "    else:\n",
    "        # TODO: add more classification metrics later\n",
    "        score_types = ['roc_auc_score', 'prc_auc_score', 'accuracy_score', 'precision', 'recall_score', 'npv', 'matthews_cc', 'kappa','cross_entropy']\n",
    "\n",
    "    subsets = ['train', 'valid', 'test']\n",
    "    score_dict = {}\n",
    "    ncmpd_dict = {}\n",
    "    for subset in subsets:\n",
    "        score_dict[subset] = {}\n",
    "        for score_type in score_types:\n",
    "            score_dict[subset][score_type] = []\n",
    "        ncmpd_dict[subset] = []\n",
    "\n",
    "    metadata_list_dict = {}\n",
    "    if result_dir:\n",
    "        if isinstance(result_dir, str):\n",
    "            result_dir = [result_dir]\n",
    "        for rd in result_dir:\n",
    "            if rd not in metadata_list_dict:\n",
    "                metadata_list_dict[rd] = []\n",
    "            for dirpath, dirnames, filenames in os.walk(rd):\n",
    "                if \"model_metadata.json\" in filenames:\n",
    "                    with open(os.path.join(dirpath, 'model_metadata.json')) as f:\n",
    "                        metadata_dict = json.load(f)\n",
    "                    metadata_list_dict[rd].append(metadata_dict)\n",
    "\n",
    "    for ss in metadata_list_dict:\n",
    "        for i, metadata_dict in enumerate(metadata_list_dict[ss]):\n",
    "            if (i % 10 == 0) and verbose:\n",
    "                print('Processing collection %s model %d' % (ss, i))\n",
    "            # Check that model has metrics before we go on\n",
    "            if not 'training_metrics' in metadata_dict:\n",
    "                continue\n",
    "            collection_list.append(ss)\n",
    "            model_uuid = metadata_dict['model_uuid']\n",
    "            model_uuid_list.append(model_uuid)\n",
    "            time_built = metadata_dict['time_built']\n",
    "            time_built_list.append(time_built)\n",
    "\n",
    "            model_params = metadata_dict['model_parameters']\n",
    "            model_type = model_params['model_type']\n",
    "            model_type_list.append(model_type)\n",
    "            prediction_type_list.append(model_params['prediction_type'])\n",
    "            featurizer = model_params['featurizer']\n",
    "            featurizer_list.append(featurizer)\n",
    "            if 'descriptor_specific' in metadata_dict:\n",
    "                desc_type = metadata_dict['descriptor_specific']['descriptor_type']\n",
    "            elif featurizer in ['graphconv', 'ecfp']:\n",
    "                desc_type = featurizer\n",
    "            else:\n",
    "                desc_type = ''\n",
    "            desc_type_list.append(desc_type)\n",
    "            dataset_key = metadata_dict['training_dataset']['dataset_key']\n",
    "            bucket = metadata_dict['training_dataset']['bucket']\n",
    "            dataset_key_list.append(dataset_key)\n",
    "            bucket_list.append(bucket)\n",
    "            dset_metadata = metadata_dict['training_dataset']['dataset_metadata']\n",
    "            param = metadata_dict['training_dataset']['response_cols'][0]\n",
    "            param_list.append(param)\n",
    "            transform_type = metadata_dict['training_dataset']['feature_transform_type']\n",
    "            transform_list.append(transform_type)\n",
    "            split_params = metadata_dict['splitting_parameters']\n",
    "            splitter_list.append(split_params['splitter'])\n",
    "            split_uuid_list.append(split_params.get('split_uuid', ''))\n",
    "            split_strategy = split_params['split_strategy']\n",
    "            split_strategy_list.append(split_strategy)\n",
    "\n",
    "            if 'umap_specific' in metadata_dict:\n",
    "                umap_params = metadata_dict['umap_specific']\n",
    "                umap_dim_list.append(umap_params['umap_dim'])\n",
    "                umap_targ_wt_list.append(umap_params['umap_targ_wt'])\n",
    "                umap_neighbors_list.append(umap_params['umap_neighbors'])\n",
    "                umap_min_dist_list.append(umap_params['umap_min_dist'])\n",
    "            else:\n",
    "                umap_dim_list.append(nan)\n",
    "                umap_targ_wt_list.append(nan)\n",
    "                umap_neighbors_list.append(nan)\n",
    "                umap_min_dist_list.append(nan)\n",
    "\n",
    "            if model_type == 'NN':\n",
    "                nn_params = metadata_dict['nn_specific']\n",
    "                max_epochs_list.append(nn_params['max_epochs'])\n",
    "                best_epoch_list.append(nn_params['best_epoch'])\n",
    "                learning_rate_list.append(nn_params['learning_rate'])\n",
    "                layer_sizes_list.append(','.join(['%d' % s for s in nn_params['layer_sizes']]))\n",
    "                dropouts_list.append(','.join(['%.2f' % d for d in nn_params['dropouts']]))\n",
    "                rf_estimators_list.append(nan)\n",
    "                rf_max_features_list.append(nan)\n",
    "                rf_max_depth_list.append(nan)\n",
    "                xgb_gamma_list.append(nan)\n",
    "                xgb_learning_rate_list.append(nan)\n",
    "            elif model_type == 'RF':\n",
    "                rf_params = metadata_dict['rf_specific']\n",
    "                rf_estimators_list.append(rf_params['rf_estimators'])\n",
    "                rf_max_features_list.append(rf_params['rf_max_features'])\n",
    "                rf_max_depth_list.append(rf_params['rf_max_depth'])\n",
    "                max_epochs_list.append(nan)\n",
    "                best_epoch_list.append(nan)\n",
    "                learning_rate_list.append(nan)\n",
    "                layer_sizes_list.append(nan)\n",
    "                dropouts_list.append(nan)\n",
    "                xgb_gamma_list.append(nan)\n",
    "                xgb_learning_rate_list.append(nan)\n",
    "            elif model_type == 'xgboost':\n",
    "                # TODO: Add xgboost parameters\n",
    "                xg_params = metadata_dict['xgb_specific']\n",
    "                xgb_gamma_list.append(xg_params['xgb_gamma'])\n",
    "                xgb_learning_rate_list.append(xg_params['xgb_learning_rate'])\n",
    "                max_epochs_list.append(nan)\n",
    "                best_epoch_list.append(nan)\n",
    "                learning_rate_list.append(nan)\n",
    "                layer_sizes_list.append(nan)\n",
    "                dropouts_list.append(nan)\n",
    "                rf_estimators_list.append(nan)\n",
    "                rf_max_features_list.append(nan)\n",
    "                rf_max_depth_list.append(nan)\n",
    "            else:\n",
    "                raise Exception('Unexpected model type %s' % model_type)\n",
    "\n",
    "            # Get model metrics for this model\n",
    "            metrics_dicts = metadata_dict['training_metrics']\n",
    "            #print(\"Got %d metrics dicts for model %s\" % (len(metrics_dicts), model_uuid))\n",
    "            subset_metrics = {}\n",
    "            for metrics_dict in metrics_dicts:\n",
    "                if metrics_dict['label'] == 'best':\n",
    "                    subset = metrics_dict['subset']\n",
    "                    subset_metrics[subset] = metrics_dict['prediction_results']\n",
    "            if split_strategy == 'k_fold_cv':\n",
    "                dset_size = subset_metrics['train']['num_compounds'] + subset_metrics['test']['num_compounds']\n",
    "            else:\n",
    "                dset_size = subset_metrics['train']['num_compounds'] + subset_metrics['valid']['num_compounds'] + subset_metrics['test']['num_compounds']\n",
    "            for subset in subsets:\n",
    "                subset_size = subset_metrics[subset]['num_compounds']\n",
    "                for score_type in score_types:\n",
    "                    try:\n",
    "                        score = subset_metrics[subset][score_type]\n",
    "                    except KeyError:\n",
    "                        score = float('nan')\n",
    "                    score_dict[subset][score_type].append(score)\n",
    "                ncmpd_dict[subset].append(subset_size)\n",
    "            dset_size_list.append(dset_size)\n",
    "\n",
    "    col_dict = dict(\n",
    "                    collection=collection_list,\n",
    "                    model_uuid=model_uuid_list,\n",
    "                    time_built=time_built_list,\n",
    "                    prediction_type=prediction_type_list,\n",
    "                    model_type=model_type_list,\n",
    "                    featurizer=featurizer_list,\n",
    "                    features=desc_type_list,\n",
    "                    transformer=transform_list,\n",
    "                    splitter=splitter_list,\n",
    "                    split_strategy=split_strategy_list,\n",
    "                    split_uuid=split_uuid_list,\n",
    "                    umap_dim=umap_dim_list,\n",
    "                    umap_targ_wt=umap_targ_wt_list,\n",
    "                    umap_neighbors=umap_neighbors_list,\n",
    "                    umap_min_dist=umap_min_dist_list,\n",
    "                    layer_sizes=layer_sizes_list,\n",
    "                    dropouts=dropouts_list,\n",
    "                    learning_rate=learning_rate_list,\n",
    "                    best_epoch=best_epoch_list,\n",
    "                    max_epochs=max_epochs_list,\n",
    "                    rf_estimators=rf_estimators_list,\n",
    "                    rf_max_features=rf_max_features_list,\n",
    "                    rf_max_depth=rf_max_depth_list,\n",
    "                    xgb_gamma = xgb_gamma_list,\n",
    "                    xgb_learning_rate = xgb_learning_rate_list,\n",
    "                    dataset_bucket=bucket_list,\n",
    "                    dataset_key=dataset_key_list,\n",
    "                    dataset_size=dset_size_list,\n",
    "                    parameter=param_list\n",
    "                    )\n",
    "\n",
    "    perf_df = pd.DataFrame(col_dict)\n",
    "    for subset in subsets:\n",
    "        ncmpds_col = '%s_size' % subset\n",
    "        perf_df[ncmpds_col] = ncmpd_dict[subset]\n",
    "        for score_type in score_types:\n",
    "            metric_col = '%s_%s' % (subset, score_type)\n",
    "            perf_df[metric_col] = score_dict[subset][score_type]\n",
    "\n",
    "    return perf_df\n",
    "\n",
    "from atomsci.ddm.pipeline import predict_from_model as pfm\n",
    "\n",
    "def predict_from_model_file(model_path, input_df, id_col='compound_id', smiles_col='rdkit_smiles',\n",
    "                     response_col=None, is_featurized=False, dont_standardize=False):\n",
    "    \"\"\"\n",
    "    Loads a pretrained model from a model tarball file and runs predictions on compounds in an input\n",
    "    data frame.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): File path of the model tarball file.\n",
    "\n",
    "        input_df (DataFrame): Input data to run predictions on; must at minimum contain SMILES strings.\n",
    "\n",
    "        id_col (str): Name of the column containing compound IDs. If none is provided, sequential IDs will be\n",
    "        generated.\n",
    "\n",
    "        smiles_col (str): Name of the column containing SMILES strings; required.\n",
    "\n",
    "        response_col (str): Name of an optional column containing actual response values; if it is provided, \n",
    "        the actual values will be included in the returned data frame to make it easier for you to assess performance.\n",
    "\n",
    "        dont_standardize (bool): By default, SMILES strings are salt-stripped and standardized using RDKit; \n",
    "        if you have already done this, or don't want them to be standardized, set dont_standardize to True.\n",
    "\n",
    "    Return: \n",
    "        A data frame with compound IDs, SMILES strings and predicted response values. Actual response values\n",
    "        will be included if response_col is provided. Standard prediction error estimates will be included\n",
    "        if the model was trained with uncertainty=True. Note that the predicted and actual response\n",
    "        columns will be labeled according to the response_col setting in the original training data,\n",
    "        not the response_col passed to this function; e.g. if the original model response_col was 'pIC50',\n",
    "        the returned data frame will contain columns 'pIC50_actual', 'pIC50_pred' and 'pIC50_std'.\n",
    "    \"\"\"\n",
    "\n",
    "    input_df, pred_params = pfm._prepare_input_data(input_df, id_col, smiles_col, response_col, dont_standardize)\n",
    "\n",
    "    has_responses = ('response_cols' in pred_params)\n",
    "    pred_params = parse.wrapper(pred_params)\n",
    "\n",
    "    pipe = mp.create_prediction_pipeline_from_file(pred_params, reload_dir=None, model_path=model_path)\n",
    "    if pipe.params.model_type == 'xgboost':\n",
    "        pipe.params.uncertainty = False\n",
    "    pred_df = pipe.predict_full_dataset(input_df, contains_responses=has_responses, is_featurized=is_featurized,\n",
    "                                        dset_params=pred_params)\n",
    "    pred_df = pred_df.sort_values(by=id_col)\n",
    "    return pred_df\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          cmap=plt.cm.Blues, ax=None):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "#     print(cm)\n",
    "    if ax is None:\n",
    "        im = plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#         plt.colorbar(cmap=cmap, shrink=0.7)\n",
    "        tick_marks = range(0,len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=0)\n",
    "        plt.yticks(tick_marks, classes, rotation=90, va='center')\n",
    "\n",
    "        fmt = '.2f' if normalize else 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "    else:\n",
    "        im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#         plt.colorbar(im, shrink=0.7)\n",
    "        fmt = '.2f' if normalize else 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "#         ax.tight_layout()\n",
    "        ax.set_ylabel('True label')\n",
    "        ax.set_xlabel('Predicted label')\n",
    "        tick_marks = range(0,len(classes))\n",
    "        ax.set_xticks(tick_marks)\n",
    "        ax.set_xticklabels(classes, rotation=0)\n",
    "        ax.set_yticks(tick_marks)\n",
    "        ax.set_yticklabels(classes, rotation=90, va='center')\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHjYDaTNBYjQ"
   },
   "source": [
    "# Visualize hyper parameter search\n",
    "- you can use regression or classification models for this, just choose a different metric for the y-axis\n",
    "- box plots work well if you did a grid search, scatter plots better if you did a HPO optimization\n",
    "- LOTS of parameters to look at, think about questions you have for the data:\n",
    "  - how do train, valid and test metrics look?\n",
    "  - which features are best for modeling the data?\n",
    "  - which hp's are the best?\n",
    "  - which models are the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UY2GIhSTa4uL"
   },
   "source": [
    "#### Get a table of performance metrics of all models in your directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSDa2HTH6wbD"
   },
   "outputs": [],
   "source": [
    "# slow the first time after connecting your google drive\n",
    "result_dir = '/content/drive/MyDrive/Columbia_E4511/HTR3A_models'\n",
    "perf_df = get_summary_perf_tables(collection_names=None, filter_dict={}, prediction_type = 'classification', result_dir=result_dir, verbose=False)\n",
    "perf_df = perf_df[perf_df.rf_estimators!=500]\n",
    "perf_df = perf_df[perf_df.prediction_type=='classification']\n",
    "perf_df.sort_values(by=\"valid_roc_auc_score\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NaZlpJQJSffk"
   },
   "outputs": [],
   "source": [
    "perf_df.groupby(by=['model_type', 'features']).count()[['model_uuid']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvvLRaxhbHzJ"
   },
   "source": [
    "#### Examine ROC_AUC scores from train, valid and test sets\n",
    "- examine other metrics instead. Do you see differences?\n",
    "- Play around with hues from different hyperparameters. Do you see any trends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_uRvu0BI-D4"
   },
   "outputs": [],
   "source": [
    "# what are the scores like for train, valid and test sets?\n",
    "\n",
    "scoretype='roc_auc_score'\n",
    "subset='valid'\n",
    "winnertype= f'{subset}_{scoretype}'\n",
    "plot_df=perf_df[[f\"train_{scoretype}\",f\"valid_{scoretype}\",f\"test_{scoretype}\"]]\n",
    "# turn off sorting if you have a ton of models.. slow\n",
    "plot_df=plot_df.sort_values(f\"valid_{scoretype}\")\n",
    "\n",
    "fig, ax = plt.subplots(1,4,figsize=(40,10))\n",
    "sns.kdeplot(perf_df[f'train_{scoretype}'], label=\"train\",ax=ax[0])\n",
    "sns.kdeplot(perf_df[f'valid_{scoretype}'], label=\"valid\",ax=ax[0])\n",
    "sns.kdeplot(perf_df[f'test_{scoretype}'], label=\"test\",ax=ax[0])\n",
    "ax[0].set_xlabel(f'{scoretype}')\n",
    "\n",
    "ax[0].legend(loc=\"upper left\")\n",
    "plot_df = perf_df[perf_df.model_type==\"RF\"]\n",
    "huefeat = 'rf_estimators'\n",
    "plot_df=plot_df[[f\"train_{scoretype}\",f\"valid_{scoretype}\",f\"test_{scoretype}\", huefeat, 'model_uuid']]\n",
    "plot_df=plot_df.sort_values(f\"valid_{scoretype}\")\n",
    "plot_df=plot_df.melt(id_vars=['model_uuid',huefeat])\n",
    "sns.lineplot(data=plot_df, x='variable', y='value', units ='model_uuid', estimator=None, hue=huefeat, legend='brief', ax = ax[1]);\n",
    "ax[1].set_ylim(perf_df[f'test_{scoretype}'].min()-.1,1)\n",
    "ax[1].set_title('RF')\n",
    "\n",
    "plot_df = perf_df[perf_df.model_type==\"NN\"]\n",
    "huefeat = 'learning_rate'\n",
    "plot_df=plot_df[[f\"train_{scoretype}\",f\"valid_{scoretype}\",f\"test_{scoretype}\", huefeat, 'model_uuid']]\n",
    "plot_df=plot_df.sort_values(f\"valid_{scoretype}\")\n",
    "plot_df=plot_df.melt(id_vars=['model_uuid',huefeat])\n",
    "sns.lineplot(data=plot_df, x='variable', y='value', units ='model_uuid', estimator=None, hue=huefeat, legend='brief', ax = ax[2]);\n",
    "ax[2].set_ylim(perf_df[f'test_{scoretype}'].min()-.1,1)\n",
    "ax[2].set_title('NN')\n",
    "\n",
    "plot_df = perf_df[perf_df.model_type==\"xgboost\"]\n",
    "huefeat = 'xgb_learning_rate'\n",
    "plot_df=plot_df[[f\"train_{scoretype}\",f\"valid_{scoretype}\",f\"test_{scoretype}\", huefeat, 'model_uuid']]\n",
    "plot_df=plot_df.sort_values(f\"valid_{scoretype}\")\n",
    "plot_df=plot_df.melt(id_vars=['model_uuid',huefeat])\n",
    "sns.lineplot(data=plot_df, x='variable', y='value', units ='model_uuid', estimator=None, hue=huefeat, legend='brief', ax = ax[3]);\n",
    "ax[3].set_ylim(perf_df[f'test_{scoretype}'].min()-.1,1)\n",
    "ax[3].set_title('xgboost')\n",
    "\n",
    "fig.suptitle(f\"{scoretype}s for HTR3A regression models\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yHDxIS0c9b-"
   },
   "source": [
    "#### Examine ROC_AUC scores for each feature set\n",
    "- what do you think of the variability among scores for different model types? Is it different for MAE or RMS scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUQXBtQPZj8p"
   },
   "outputs": [],
   "source": [
    "# which feature set works best for each model type?\n",
    "\n",
    "fig,ax = plt.subplots(1,3,sharey=True, figsize=(26,8))\n",
    "plot_df = perf_df[perf_df.model_type=='RF']\n",
    "sns.boxplot(data=plot_df, x=\"features\", y=f\"valid_{scoretype}\", width = 0.8, ax=ax[0]);\n",
    "# plot_df = perf_df[perf_df.model_type=='NN']\n",
    "# sns.boxplot(data=plot_df, x=\"features\", y=f\"valid_{scoretype}\", width = 0.8, ax=ax[1]);\n",
    "# plot_df = perf_df[perf_df.model_type=='xgboost']\n",
    "# sns.boxplot(data=plot_df, x=\"features\", y=f\"valid_{scoretype}\", width = 0.8, ax=ax[2]);\n",
    "ax[0].set_title(\"RF\");\n",
    "ax[1].set_title(\"NN\");\n",
    "ax[2].set_title(\"XG\");\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnvptW5T0mKK"
   },
   "source": [
    "#### Examine the effect of RF hyperparameters on model performance\n",
    "- any major differences between feature sets?\n",
    "- did you capture the right range of HPs or do you need to expand / zero in on a particular range?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bVXltSu6z0f"
   },
   "outputs": [],
   "source": [
    "modtype = 'RF'\n",
    "feat1 = 'rf_estimators'\n",
    "feat2 = 'rf_max_depth'\n",
    "feat3 = 'rf_max_features'\n",
    "sub_df = perf_df[perf_df.model_type==modtype]\n",
    "\n",
    "fig, ax = plt.subplots(3,3,sharey=True, figsize=(26,18))\n",
    "plot_df=sub_df[sub_df.features=='ecfp']\n",
    "sns.boxplot(data=plot_df, x=feat1, y=f\"valid_{scoretype}\", ax=ax[0,0])\n",
    "sns.boxplot(data=plot_df, x=feat2, y=f\"valid_{scoretype}\", ax=ax[0,1])\n",
    "ax[0,1].set_title(f\"{modtype} - {plot_df.features.iloc[0]}\")\n",
    "sns.boxplot(data=plot_df, x=feat3, y=f\"valid_{scoretype}\", ax=ax[0,2])\n",
    "\n",
    "plot_df=sub_df[sub_df.features=='mordred_filtered']\n",
    "sns.boxplot(data=plot_df, x=feat1, y=f\"valid_{scoretype}\", ax=ax[1,0])\n",
    "sns.boxplot(data=plot_df, x=feat2, y=f\"valid_{scoretype}\", ax=ax[1,1])\n",
    "ax[1,1].set_title(f\"{modtype} - {plot_df.features.iloc[0]}\")\n",
    "sns.boxplot(data=plot_df, x=feat3, y=f\"valid_{scoretype}\", ax=ax[1,2])\n",
    "\n",
    "plot_df=sub_df[sub_df.features=='rdkit_raw']\n",
    "sns.boxplot(data=plot_df, x=feat1, y=f\"valid_{scoretype}\", ax=ax[2,0])\n",
    "sns.boxplot(data=plot_df, x=feat2, y=f\"valid_{scoretype}\", ax=ax[2,1])\n",
    "ax[2,1].set_title(f\"{modtype} - {plot_df.features.iloc[0]}\")\n",
    "sns.boxplot(data=plot_df, x=feat3, y=f\"valid_{scoretype}\", ax=ax[2,2])\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Yl3HitD0tQ5"
   },
   "source": [
    "#### Examine NN hyperparameters\n",
    "- are training or test sets different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hy9I-HiWKfIG"
   },
   "outputs": [],
   "source": [
    "# i didn't train any NN classification models\n",
    "\n",
    "# modtype = 'NN'\n",
    "# feat1 = 'dropouts'\n",
    "# feat2 = 'learning_rate'\n",
    "# feat3 = 'layer_sizes'\n",
    "# sub_df = perf_df[perf_df.model_type==modtype]\n",
    "\n",
    "# fig, ax = plt.subplots(3,3,sharey=True,figsize=(26,18))\n",
    "# plot_df=sub_df[sub_df.features=='ecfp']\n",
    "# sns.boxplot(data=plot_df, x=feat1, y=f\"valid_{scoretype}\", ax=ax[0,0])\n",
    "# sns.boxplot(data=plot_df, x=feat2, y=f\"valid_{scoretype}\", ax=ax[0,1])\n",
    "# ax[0,1].set_title(f\"{modtype} - {plot_df.features.iloc[0]}\")\n",
    "# sns.boxplot(data=plot_df, x=feat3, y=f\"valid_{scoretype}\", ax=ax[0,2])\n",
    "\n",
    "# plot_df=sub_df[sub_df.features=='mordred_filtered']\n",
    "# sns.boxplot(data=plot_df, x=feat1, y=f\"valid_{scoretype}\", ax=ax[1,0])\n",
    "# sns.boxplot(data=plot_df, x=feat2, y=f\"valid_{scoretype}\", ax=ax[1,1])\n",
    "# ax[1,1].set_title(f\"{modtype} - {plot_df.features.iloc[0]}\")\n",
    "# sns.boxplot(data=plot_df, x=feat3, y=f\"valid_{scoretype}\", ax=ax[1,2])\n",
    "\n",
    "# plot_df=sub_df[sub_df.features=='rdkit_raw']\n",
    "# sns.boxplot(data=plot_df, x=feat1, y=f\"valid_{scoretype}\", ax=ax[2,0])\n",
    "# sns.boxplot(data=plot_df, x=feat2, y=f\"valid_{scoretype}\", ax=ax[2,1]); ax[2,1].tick_params(axis='x', rotation=45);\n",
    "# ax[2,1].set_title(f\"{modtype} - {plot_df.features.iloc[0]}\")\n",
    "# sns.boxplot(data=plot_df, x=feat3, y=f\"valid_{scoretype}\", ax=ax[2,2])\n",
    "# fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjOQkIQj02OD"
   },
   "source": [
    "#### Examine XGBoost hyperparameters\n",
    "- what is the third graph?\n",
    "- how do scores compare when plotted against each other?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fd9d7RsGOrkr"
   },
   "outputs": [],
   "source": [
    "# i didn't train xgboost classification models\n",
    "\n",
    "# modtype = 'xgboost'\n",
    "# feat1 = 'xgb_gamma'\n",
    "# feat2 = 'xgb_learning_rate'\n",
    "# feat3 = 'valid_mae_score'\n",
    "# sub_df = perf_df[perf_df.model_type==modtype]\n",
    "\n",
    "# fig, ax = plt.subplots(3,3,sharey=True, figsize=(26,18))\n",
    "# plot_df=sub_df[sub_df.features=='ecfp']\n",
    "# sns.boxplot(data=plot_df, x=feat1, y=f\"valid_{scoretype}\", ax=ax[0,0])\n",
    "# sns.boxplot(data=plot_df, x=feat2, y=f\"valid_{scoretype}\", ax=ax[0,1])\n",
    "# ax[0,1].set_title(f\"{modtype} - {plot_df.features.iloc[0]}\")\n",
    "# sns.scatterplot(data=plot_df, x=feat3, y=f\"valid_{scoretype}\", ax=ax[0,2])\n",
    "\n",
    "# plot_df=sub_df[sub_df.features=='mordred_filtered']\n",
    "# sns.boxplot(data=plot_df, x=feat1, y=f\"valid_{scoretype}\", ax=ax[1,0])\n",
    "# sns.boxplot(data=plot_df, x=feat2, y=f\"valid_{scoretype}\", ax=ax[1,1])\n",
    "# ax[1,1].set_title(f\"{modtype} - {plot_df.features.iloc[0]}\")\n",
    "# sns.scatterplot(data=plot_df, x=feat3, y=f\"valid_{scoretype}\", ax=ax[1,2])\n",
    "\n",
    "# plot_df=sub_df[sub_df.features=='rdkit_raw']\n",
    "# sns.boxplot(data=plot_df, x=feat1, y=f\"valid_{scoretype}\", ax=ax[2,0])\n",
    "# sns.boxplot(data=plot_df, x=feat2, y=f\"valid_{scoretype}\", ax=ax[2,1]); ax[2,1].tick_params(axis='x', rotation=45);\n",
    "# ax[2,1].set_title(f\"{modtype} - {plot_df.features.iloc[0]}\")\n",
    "# sns.scatterplot(data=plot_df, x=feat3, y=f\"valid_{scoretype}\", ax=ax[2,2])\n",
    "# fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wA0R-c7-yDil"
   },
   "source": [
    "# Choose best model and examine predictions (classification models)\n",
    "Metrics to assess classification models: (google definitions)\n",
    "- cross_entropy\n",
    "- kappa\n",
    "- matthews_cc\n",
    "- npv\n",
    "- prc_auc_score\n",
    "- precision\n",
    "- recall_score\n",
    "- roc_auc_score\n",
    "- balanced_accuracy\n",
    "\n",
    "### Challenge: try adding balanced accuracy scores into this plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlzn34kmzCH1"
   },
   "source": [
    "#### select best models & visualize metrics with radar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l35TSawFyQwd"
   },
   "outputs": [],
   "source": [
    "# best models per metric\n",
    "top_xnt_model = perf_df[perf_df.valid_cross_entropy==perf_df.valid_cross_entropy.min()] # minimize cross entropy, maximize rest\n",
    "top_kap_model = perf_df[perf_df.valid_kappa==perf_df.valid_kappa.max()]\n",
    "top_mcc_model = perf_df[perf_df.valid_matthews_cc==perf_df.valid_matthews_cc.max()]\n",
    "top_npv_model = perf_df[perf_df.valid_npv==perf_df.valid_npv.max()]\n",
    "top_prc_model = perf_df[perf_df.valid_prc_auc_score==perf_df.valid_prc_auc_score.max()]\n",
    "top_pre_model = perf_df[perf_df.valid_precision==perf_df.valid_precision.max()]\n",
    "top_rec_model = perf_df[perf_df.valid_recall_score==perf_df.valid_recall_score.max()]\n",
    "top_roc_model = perf_df[perf_df.valid_roc_auc_score==perf_df.valid_roc_auc_score.max()] \n",
    "\n",
    "topmods = pd.concat([top_xnt_model, top_kap_model, top_mcc_model, top_npv_model, top_prc_model, top_pre_model, top_rec_model, top_roc_model])\n",
    "# note: fewer different model_uuids here than metrics from some models winning at more than one metric\n",
    "topmods = topmods.drop_duplicates()\n",
    "topmods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RV07vPkr3IFF"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "plot_df=topmods\n",
    "categories = ['valid_roc_auc_score','valid_prc_auc_score','valid_accuracy_score','valid_precision', 'valid_recall_score', 'valid_npv', 'valid_matthews_cc', 'valid_kappa', 'valid_cross_entropy']\n",
    "# categories_2 = ['best_train_r2_score','best_valid_r2_score','best_train_mae_score','best_valid_mae_score','best_train_rms_score',  'best_valid_rms_score']\n",
    "N=len(categories)\n",
    "angles = [n / float(N) * 2 * math.pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,10), subplot_kw=dict(polar=True))\n",
    "ax.set_theta_offset(math.pi / 2)\n",
    "ax.set_theta_direction(-1)\n",
    "ax.set_rlabel_position(0)\n",
    "ax.set_yticks([.25,0.5,0.75,1])\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "for ilocn in plot_df.index:\n",
    "    values=plot_df.loc[ilocn][categories].values.flatten().tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, linewidth=2, linestyle='solid', label=f\"Model # {ilocn}\")\n",
    "#         ax.fill(angles, values, 'b', alpha=0.05)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "ax.set_title(f\"Top model metrics\", y=1.06);\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDQ-t3FMGk6b"
   },
   "source": [
    "#### select models and generate predictions\n",
    "- predict on the same dataset as the model was trained on first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iuX8-MfUGsc9"
   },
   "outputs": [],
   "source": [
    "from atomsci.ddm.pipeline import predict_from_model as pfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHH1uMFLH_ZC"
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i, mod in topmods.iterrows():\n",
    "  model_path = f\"{mod.collection}/{mod.dataset_key.split('/')[-1].strip('.csv')}_model_{mod.model_uuid}.tar.gz\"\n",
    "  split_df = pd.read_csv(f\"/content/drive/MyDrive/Columbia_E4511/HTR3A_curated_train_valid_test_scaffold_{mod.split_uuid}.csv\")\n",
    "  if mod.features in ['rdkit_raw', 'mordred_filtered']:\n",
    "    feat_df = pd.read_csv(f'/content/drive/MyDrive/Columbia_E4511/scaled_descriptors/HTR3A_curated_with_{mod.features}_descriptors.csv')\n",
    "    feat_df = feat_df[~feat_df.VALUE_NUM_mean.isna()]\n",
    "    feat_df = feat_df[feat_df.VALUE_NUM_mean>2]\n",
    "    is_featurized = True\n",
    "  else:\n",
    "    feat_df = pd.read_csv(mod.dataset_key)\n",
    "    feat_df = feat_df[~feat_df.VALUE_NUM_mean.isna()]\n",
    "    feat_df = feat_df[feat_df.VALUE_NUM_mean>2]\n",
    "    is_featurized = False\n",
    "  pred_df = pfm.predict_from_model_file(model_path= model_path, input_df=feat_df, id_col='compound_id', smiles_col = 'base_rdkit_smiles', \n",
    "                        response_col = 'active', is_featurized = is_featurized, dont_standardize=True)\n",
    "  pred_df = pd.merge(pred_df, split_df, left_on='compound_id', right_on='cmpd_id')\n",
    "  pred_df = pred_df.rename(columns={'active_actual':'active'})\n",
    "  preds.append(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwsN6neIQab-"
   },
   "outputs": [],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeusuXyXcZoy"
   },
   "source": [
    "#### Plot confusion matrices and other plots\n",
    "- confusion matrix (active vs inactive)\n",
    "- reciever-operating curve\n",
    "  - what does a \"good\" curve look like?\n",
    "- precision-recall curve\n",
    "  - what does a \"good\" curve look like?\n",
    "- when is it appropriate to use predicted class versus class probabilities?\n",
    "- what do the graphs look like if you change to _pred from _prob?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Km3qDJXdewP"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc, roc_auc_score, r2_score, precision_recall_curve, average_precision_score, confusion_matrix, precision_score, recall_score, balanced_accuracy_score\n",
    "import itertools\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzSxz8UkHCqI"
   },
   "outputs": [],
   "source": [
    "full_df=preds[0]\n",
    "model_uuid = topmods.model_uuid.tolist()[0]\n",
    "response_col = 'active'\n",
    "# graph\n",
    "fig, ax = plt.subplots(2,2, sharex=False, sharey=False, figsize=(15,15))\n",
    "ax=ax.ravel()\n",
    "\n",
    "# training set    \n",
    "pred_df=full_df[full_df.subset=='train']\n",
    "# roc curve\n",
    "roc_auc = roc_auc_score(pred_df[f'{response_col}'], pred_df[f'{response_col}_prob'])\n",
    "fpr, tpr, _ = roc_curve(pred_df[f'{response_col}'], pred_df[f'{response_col}_prob'])\n",
    "lw = 2\n",
    "ax[0].plot(fpr, tpr, lw=lw, label='Train ROC AUC = %0.2f' % roc_auc, color = pal[3])\n",
    "# prc \n",
    "average_precision = average_precision_score(pred_df[f'{response_col}'], pred_df[f'{response_col}_prob'])\n",
    "precision, recall, _ = precision_recall_curve(pred_df[f'{response_col}'], pred_df[f'{response_col}_prob'])\n",
    "line_kwargs = {\"drawstyle\": \"steps-post\"}\n",
    "line_kwargs[\"label\"] = (f\"Train AP = \"\n",
    "                        f\"{average_precision:0.2f}\")\n",
    "ax[1].plot(recall, precision, color=pal[3], **line_kwargs) \n",
    "\n",
    "# validation set\n",
    "pred_df=full_df[full_df.subset=='valid']\n",
    "# ROC AUC curve\n",
    "roc_auc = roc_auc_score(pred_df[f'{response_col}'], pred_df[f'{response_col}_prob'])\n",
    "fpr, tpr, _ = roc_curve(pred_df[f'{response_col}'], pred_df[f'{response_col}_prob'])\n",
    "lw = 2\n",
    "line_kwargs = {\"drawstyle\": \"steps-post\"}\n",
    "ax[0].plot(fpr, tpr, lw=lw, label='Valid ROC AUC = %0.2f' % roc_auc, color = pal[4], **line_kwargs)\n",
    "# PR curve\n",
    "average_precision = average_precision_score(pred_df[f'{response_col}'], pred_df[f'{response_col}_prob'])\n",
    "precision, recall, _ = precision_recall_curve(pred_df[f'{response_col}'], pred_df[f'{response_col}_prob'])\n",
    "line_kwargs = {\"drawstyle\": \"steps-post\"}\n",
    "line_kwargs[\"label\"] = (f\"Valid AP = \"\n",
    "                        f\"{average_precision:0.2f}\")\n",
    "ax[1].plot(recall, precision, color=pal[4], **line_kwargs)\n",
    "# confusion matrix - valid set\n",
    "cm=confusion_matrix(pred_df[f'{response_col}'], pred_df[f'{response_col}_pred'])\n",
    "class_names = ['inactive','active']\n",
    "im = plot_confusion_matrix(cm, classes=class_names, normalize=False, cmap=sns.cubehelix_palette(rot=0, start=2.40, as_cmap=True), ax=ax[2])\n",
    "fig.colorbar(mappable=im, ax=ax[2], shrink=0.7)\n",
    "ax[2].set_title(\"Valid CM\")\n",
    "\n",
    "# test set    \n",
    "pred_df=full_df[full_df.subset=='test']\n",
    "# ROC AUC curve\n",
    "roc_auc = roc_auc_score(pred_df[f'{response_col}'], pred_df[f'{response_col}_prob'])\n",
    "fpr, tpr, _ = roc_curve(pred_df[f'{response_col}'], pred_df[f'{response_col}_prob'])\n",
    "lw = 2\n",
    "line_kwargs = {\"drawstyle\": \"steps-post\"}\n",
    "ax[0].plot(fpr, tpr, lw=lw, label=' Test ROC AUC = %0.2f' % roc_auc, color = pal[5], **line_kwargs)\n",
    "ax[0].plot([0, 1], [0, 1], lw=lw, linestyle='--')\n",
    "ax[0].set_xlim([-0.05, 1.05])\n",
    "ax[0].set_ylim([-0.05, 1.05])\n",
    "ax[0].set_xlabel('False Positive Rate')\n",
    "ax[0].set_ylabel('True Positive Rate')\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "# PR curve\n",
    "average_precision = average_precision_score(pred_df[f'{response_col}'], pred_df[f'{response_col}_prob'])\n",
    "precision, recall, _ = precision_recall_curve(pred_df[f'{response_col}'], pred_df[f'{response_col}_prob'])\n",
    "line_kwargs = {\"drawstyle\": \"steps-post\"}\n",
    "line_kwargs[\"label\"] = (f\" Test AP = \"\n",
    "                        f\"{average_precision:0.2f}\")\n",
    "ax[1].plot(recall, precision, color=pal[5], **line_kwargs)\n",
    "pos_label=1\n",
    "info_pos_label = (f\" (Positive label: {pos_label})\")\n",
    "xlabel = \"Recall\" + info_pos_label\n",
    "ylabel = \"Precision\" + info_pos_label\n",
    "ax[1].set(xlabel=xlabel, ylabel=ylabel)\n",
    "ax[1].set_xlim([-0.05, 1.05])\n",
    "ax[1].set_ylim([-0.05, 1.05])\n",
    "ax[1].legend(loc=\"lower right\")\n",
    "# confusion matrix\n",
    "cm=confusion_matrix(pred_df[f'{response_col}'], pred_df[f'{response_col}_pred'])\n",
    "im=plot_confusion_matrix(cm, classes=class_names, normalize=False, cmap=sns.cubehelix_palette(rot=0, start=2.40, as_cmap=True), ax=ax[3])\n",
    "fig.colorbar(mappable=im, ax=ax[3], shrink=0.7)\n",
    "ax[3].set_title(\"Test CM\")\n",
    "plt.tight_layout()\n",
    "fig.suptitle(f'{model_uuid}', y=1.01);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YivdmG-3ktYt"
   },
   "source": [
    "# Predict on new data (classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pf_VaIanJsf"
   },
   "source": [
    "#### Generate predictions on a list of compounds from another target (such as provided by your partners)\n",
    "- there will be activity values associated with this target. You can explore overlaps between the two targets\n",
    "- here, my best models are predicting activity of the serotonin receptor, HTR3A\n",
    "- i will compare with data from the serotonin transporter, SLC6A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uocZdp49ktHs"
   },
   "outputs": [],
   "source": [
    "from atomsci.ddm.utils import curate_data\n",
    "from atomsci.ddm.utils import struct_utils as su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jg1x0qF8mdBb"
   },
   "outputs": [],
   "source": [
    "# new data must have curated smiles strings, with rdkit standardization, featurizations, etc.\n",
    "newcmpds = pd.read_csv(\"/content/drive/MyDrive/Columbia_E4511/SLC6A4_chembl.csv\", sep=\";\")\n",
    "newcmpds = curate_data.average_and_remove_duplicates('pChEMBL Value', 100, False, data=newcmpds, compound_id='Molecule ChEMBL ID', smiles_col='Smiles')\n",
    "newcmpds['base_rdkit_smiles']=su.base_smiles_from_smiles(newcmpds.Smiles.tolist())\n",
    "newcmpds = newcmpds.rename(columns = {'Molecule ChEMBL ID':'compound_id'})\n",
    "newcmpds = curate_data.aggregate_assay_data(newcmpds, value_col='VALUE_NUM_mean', output_value_col='VALUE_NUM_mean', label_actives=True, active_thresh=7, id_col='compound_id', smiles_col='base_rdkit_smiles', relation_col = 'Standard Relation')\n",
    "newcmpds = newcmpds[~newcmpds.base_rdkit_smiles.isna()]\n",
    "newcmpds = newcmpds[~newcmpds.VALUE_NUM_mean.isna()]\n",
    "newcmpds = newcmpds[newcmpds.VALUE_NUM_mean!='']\n",
    "newcmpds.to_csv(\"/content/drive/MyDrive/Columbia_E4511/SLC6A4_chembl_cur.csv\", index = False)\n",
    "newcmpds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkWaX_hRwKWc"
   },
   "outputs": [],
   "source": [
    "# next cells will be very slow if you have mordred features. \n",
    "# Get the featurized data first and then say featurized = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVg9P8FTmc4t"
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "# for i, mod in topmods.iloc[1,:].iterrows(): \n",
    "# run with one ecfp model for demo\n",
    "mod = topmods.iloc[0,:]\n",
    "model_path = f\"{mod.collection}/{mod.dataset_key.split('/')[-1].strip('.csv')}_model_{mod.model_uuid}.tar.gz\"\n",
    "pred_df = pfm.predict_from_model_file(model_path= model_path, input_df=newcmpds, id_col='compound_id', smiles_col = 'base_rdkit_smiles', \n",
    "                      response_col = 'active', is_featurized = False, dont_standardize=True)\n",
    "pred_df = pred_df.rename(columns={'active_actual':'active'})\n",
    "preds.append(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DHza8Ggmcwh"
   },
   "outputs": [],
   "source": [
    "# remember - active here is the IC50 values for other target SLC6A4 (that's where the dataset comes from)\n",
    "# active_pred is your predictions for YOUR target (HTR3A), not the other target\n",
    "# SLC6A4 is a serotonin transporter\n",
    "# HTR3A is a serotonin receptor\n",
    "# related targets, but not the same. Will be interesting to see overlaps in activity\n",
    "preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiCmFkLJ7PQN"
   },
   "source": [
    "#### Visualize predictions that don't have ground truth associated with them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftFbsKJ6VZmF"
   },
   "outputs": [],
   "source": [
    "# active prob hist\n",
    "fig, ax = plt.subplots(1, sharex=True, sharey=True, figsize = (10,10))\n",
    "preds[0].hist('active_prob', bins=10, ax=ax)\n",
    "ax.set_title(topmods.model_uuid.tolist()[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mnt8Pt-hmcnc"
   },
   "outputs": [],
   "source": [
    "# confusion matrix - your target predictions vs other target classes\n",
    "cm=confusion_matrix(preds[0][f'{response_col}'], preds[0][f'{response_col}_pred'])\n",
    "fig, ax = plt.subplots(1,figsize=(10,10))\n",
    "class_names = ['inactive','active']\n",
    "im = plot_confusion_matrix(cm, classes=class_names, normalize=False, cmap=sns.cubehelix_palette(rot=0, start=2.40, as_cmap=True), ax=ax)\n",
    "fig.colorbar(mappable=im, ax=ax, shrink=0.7)\n",
    "ax.set_xlabel('Predicted - my target (HTR3A)')\n",
    "ax.set_ylabel('True label - other target (SLC6A4)')\n",
    "ax.set_title(\"2-target CM\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBIsK_xJWgBS"
   },
   "source": [
    "#### Tanimoto distance from train to new compounds\n",
    "- what does a tanimoto distance of zero represent?\n",
    "- what do you think about the distances between your training data and the new data? are the compounds similar or different?\n",
    "- what do you think this means for the uncertainty of new predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9sKejut7-xy"
   },
   "outputs": [],
   "source": [
    "from atomsci.ddm.pipeline import chem_diversity as cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3xX4_gNXUQu"
   },
   "outputs": [],
   "source": [
    "htr3a=h1\n",
    "calc_type='nearest'\n",
    "dist_metric='tanimoto'\n",
    "smiles_lst2=newcmpds.base_rdkit_smiles.tolist()\n",
    "htr3a=htr3a.merge(split_df, left_on='compound_id', right_on='cmpd_id')\n",
    "smiles_lst1=htr3a[htr3a.subset=='train'].base_rdkit_smiles.tolist()\n",
    "dists=cd.calc_dist_smiles('ECFP',dist_metric,smiles_lst2,smiles_lst1,calc_type)\n",
    "distsdf=pd.DataFrame([smiles_lst2,list(dists)], columns=range(len(smiles_lst2)), index=['smiles','dists']).T\n",
    "df=newcmpds\n",
    "df=df.merge(distsdf, left_on='base_rdkit_smiles', right_on='smiles')\n",
    "print(len(set(smiles_lst2)-set(smiles_lst1)))\n",
    "print(list(df.loc[df.dists==0, 'compound_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oW9P5wasVxDl"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, figsize=(10,10))\n",
    "sns.distplot(dists, ax=axes, bins=np.arange(-0.05,1,0.05))\n",
    "axes.set_title(\"Tanimoto distance of compounds to training set of best models\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxDUw6wdFDH6"
   },
   "source": [
    "# Explore the domain of applicability of your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfXL5qPfTGsA"
   },
   "source": [
    "#### Make sure you have featurized data to use for UMAPs\n",
    "graphconv you can't do umaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtbCZY0oSVbo"
   },
   "outputs": [],
   "source": [
    "newcmpds.to_csv(\"/content/drive/MyDrive/Columbia_E4511/SLC6A4_chembl_cur.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_2PC8VRFCiM"
   },
   "outputs": [],
   "source": [
    "# to create umaps on training data, you need features for the model type.\n",
    "# rdkit and mordred should already be featurized if you built models with them\n",
    "# ecfp doesn't have saved features, so extract them now for the training data and new data\n",
    "# just change the train_file to featurize the other dataset\n",
    "from atomsci.ddm.pipeline import featurization as feat\n",
    "\n",
    "train_file = \"/content/drive/MyDrive/Columbia_E4511/SLC6A4_chembl_cur.csv\"\n",
    "response_col = \"VALUE_NUM_mean\"\n",
    "compound_id = \"compound_id\"\n",
    "smiles_col = \"base_rdkit_smiles\"\n",
    "result_dir = \"/content/drive/MyDrive/Columbia_E4511/HTR3A_models\"\n",
    "params = {\n",
    "        \"system\": \"LC\",\n",
    "        \"lc_account\": 'None',\n",
    "        \"datastore\": \"False\",\n",
    "        \"save_results\": \"False\",\n",
    "        \"data_owner\": \"username\",\n",
    "        \"dataset_key\": train_file,\n",
    "        \"id_col\": compound_id,\n",
    "        \"smiles_col\": smiles_col,\n",
    "        \"response_cols\": response_col,\n",
    "        \"previously_split\": \"False\",\n",
    "        \"split_only\": \"True\",\n",
    "        # \"model_type\": \"RF\",\n",
    "        \"verbose\": \"True\",\n",
    "        \"transformers\": \"True\",\n",
    "        'max_epochs': '70',\n",
    "        \"rerun\": \"False\",\n",
    "        \"result_dir\": result_dir,\n",
    "        \"featurizer\":\"ecfp\"\n",
    "    }\n",
    "pparams = parse.wrapper(params)\n",
    "MP = mp.ModelPipeline(pparams)\n",
    "featurization=None\n",
    "# comment out this line after splitting once so you don't re-split\n",
    "MP.run_mode = 'training'\n",
    "MP.params.split_only = True\n",
    "MP.params.previously_split = False\n",
    "if featurization is None:\n",
    "    featurization = feat.create_featurization(MP.params)\n",
    "MP.featurization = featurization\n",
    "MP.load_featurize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtDpWji1Q7_k"
   },
   "outputs": [],
   "source": [
    "ecfp = MP.data.dataset.X\n",
    "y=MP.data.dataset.y\n",
    "ids=MP.data.dataset.ids\n",
    "yids = [ids, y]\n",
    "yids.extend(ecfp.T)\n",
    "ecfpfeats = pd.DataFrame(yids).T\n",
    "ecfp_colnames = ['compound_id', 'mol_wt']\n",
    "col2 = list(range(0,1024))\n",
    "col2 = ['ecfp_'+ str(x) for x in col2]\n",
    "ecfp_colnames.extend(col2)\n",
    "ecfpfeats.columns = ecfp_colnames\n",
    "ecfpfeats.to_csv('/content/drive/MyDrive/Columbia_E4511/scaled_descriptors/SLC6A4_chembl_cur_with_ecfp_descriptors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NU4Q5r5Ca9s-"
   },
   "source": [
    "#### UMAP projection of new compounds onto the training dataset\n",
    "- create umap of training data\n",
    "- use the mapper object to project new compounds onto the training data\n",
    "- visualize prediction uncertainty or ADI as hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7W_UyvbZACR"
   },
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bAlfR82ZYZ8"
   },
   "outputs": [],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aU11SnzpZJGq"
   },
   "outputs": [],
   "source": [
    "cols_dict={\n",
    "    'ecfp':('ecfp_0','ecfp_1023'),\n",
    "    'mordred_filtered':('ABC','SsBr'),\n",
    "    'rdkit_raw':('MaxEStateIndex','fr_urea'),\n",
    "}\n",
    "# features model was created on, for training data\n",
    "feat_type = 'ecfp'\n",
    "feats = pd.read_csv(f\"/content/drive/MyDrive/Columbia_E4511/scaled_descriptors/HTR3A_curated_with_{feat_type}_descriptors.csv\")\n",
    "valuecol=[f'active']\n",
    "feats = feats.merge(split_df, left_on='compound_id', right_on='cmpd_id')\n",
    "feats = feats.merge(h1)\n",
    "feats = feats[feats.subset=='train']\n",
    "feats = feats.dropna(subset = valuecol).reset_index(drop=True)\n",
    "# select only the non-na feature columns to run umap on\n",
    "firstcol, lastcol=cols_dict[feat_type]\n",
    "maptrain = feats.loc[:,firstcol:lastcol]\n",
    "maptrain = maptrain.dropna(axis='columns')\n",
    "maptraincols = maptrain.columns\n",
    "# get same feature columns from new compound df\n",
    "featnew=pd.read_csv(f\"/content/drive/MyDrive/Columbia_E4511/scaled_descriptors/SLC6A4_chembl_cur_with_{feat_type}_descriptors.csv\", index_col=False)\n",
    "featnew=featnew.merge(preds[0])\n",
    "mapnew = featnew.loc[:,firstcol:lastcol]\n",
    "mapnew = mapnew.dropna(axis='columns')\n",
    "mapnewcols = mapnew.columns\n",
    "allcols = list(set(maptraincols).intersection(set(mapnewcols)))\n",
    "maptrain = maptrain[allcols]\n",
    "mapnew= mapnew[allcols]\n",
    "# create umap on training data\n",
    "mapper=umap.UMAP(n_neighbors=15, n_components=2, metric='jaccard', random_state=42).fit(maptrain)\n",
    "maptrain_coords = pd.DataFrame(mapper.embedding_, columns=('UMAP_X', 'UMAP_Y'))\n",
    "feats=pd.concat([feats,maptrain_coords], axis=1)\n",
    "# use mapper to create umap coords for new data\n",
    "mapnew_coords = pd.DataFrame(mapper.transform(mapnew), columns = ('UMAP_X', 'UMAP_Y'))\n",
    "featnew=pd.concat([featnew,mapnew_coords], axis=1)\n",
    "featnew=featnew.merge(newcmpds)\n",
    "featnew=featnew.sort_values('active')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc88YGDXaUk_"
   },
   "source": [
    "##### How would you interpret the probability output? (active_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLRArnKZeLKk"
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, ax = plt.subplots(1,2, figsize=(30,15), gridspec_kw={'width_ratios': [1, 1.2]})\n",
    "# plot with active / inactive labels\n",
    "sns.scatterplot(x=feats['UMAP_X'], y=feats[\"UMAP_Y\"], s=50, hue=feats['active'].map({0:False, 1:True}), palette=['#e0e0e0','#b3b3b3'], ax=ax[0])\n",
    "sns.scatterplot(x=featnew['UMAP_X'], y=featnew[\"UMAP_Y\"], hue=featnew[f'active_pred'], palette='Set2', marker = '^', legend='full', ax=ax[0])\n",
    "handles, _ = ax[0].get_legend_handles_labels()\n",
    "ax[0].legend([handles[1],handles[-1]], [f'Training Hits', f'Pred Hits'], fontsize=12)\n",
    "ax[0].set_title(f'Hits on {feat_type} training data UMAP (n={len(feats)})')\n",
    "# plot with continuous labels\n",
    "valcol=f'active_prob'\n",
    "# reduce number of points to plot\n",
    "plotnew = featnew[(featnew[valcol]<0.05) | (featnew[valcol]>0.8)] # highest and lowest values = more certain\n",
    "norm = plt.Normalize(plotnew[valcol].min(), plotnew[valcol].max())\n",
    "sm = plt.cm.ScalarMappable(cmap=\"viridis\", norm=norm)\n",
    "sm.set_array([])\n",
    "sns.scatterplot(x=feats['UMAP_X'], y=feats[\"UMAP_Y\"], s=50, hue=feats['active'], palette=['#e0e0e0','#b3b3b3'], ax=ax[1])\n",
    "sns.scatterplot(x=plotnew['UMAP_X'], y=plotnew[\"UMAP_Y\"], hue=plotnew[valcol], palette='viridis', marker = '^', ax=ax[1]);\n",
    "# Remove the legend and add a colorbar\n",
    "ax[1].get_legend().remove()\n",
    "ax[1].figure.colorbar(sm)\n",
    "ax[1].set_title(f'Compound cert. on {feat_type} training data UMAP (n={len(feats)})');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LeXQXYRSame9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "09_AMPL_EDA_Part2_Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
