{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "In this tutorial we demonstrate the following:\n",
    "- Build a parameter dictionary to perform a `hyperparameter optimization` for a random forest using `Bayesian optimization`.\n",
    "- Perform the optimization process.\n",
    "- Review the results\n",
    "\n",
    "We will use these **[AMPL](https://github.com/ATOMScience-org/AMPL)** functions here:\n",
    "- [parse_params](https://ampl.readthedocs.io/en/latest/utils.html#utils.hyperparam_search_wrapper.parse_params)\n",
    "- [build_search](https://ampl.readthedocs.io/en/latest/utils.html#utils.hyperparam_search_wrapper.build_search)\n",
    "- [run_search](https://ampl.readthedocs.io/en/latest/utils.html#utils.hyperparam_search_wrapper.HyperOptSearch.run_search)\n",
    "- [get_filesystem_perf_results](https://ampl.readthedocs.io/en/latest/pipeline.html#pipeline.compare_models.get_filesystem_perf_results)\n",
    "\n",
    "`Hyperparameters` dictate the parameters of the training process and the architecture of the model itself. For example, the \n",
    "number of random trees is a hyperparameter for a random forest. In contrast, a learned parameter for a random forest is the set of features that is contained in a single node (in a single tree) and the cutoff values for each of those features that determines how the data is split at that node. A full discussion of hyperparameter optimization can be found on **[wikipedia](https://en.wikipedia.org/wiki/Hyperparameter_optimization)**.\n",
    "\n",
    "The choice for hyperparameters strongly influence model performance,\n",
    "so it is important to be able to optimize them as well. **[AMPL](https://github.com/ATOMScience-org/AMPL)**\n",
    "offers a variety of hyperparameter optimization methods including\n",
    "random sampling, grid search, and Bayesian optimization. Further information for **[AMPL](https://github.com/ATOMScience-org/AMPL)**'s `Bayesian optimization` can be found **[here](https://github.com/ATOMScience-org/AMPL#hyperparameter-optimization)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup directories\n",
    "Describe important features like descriptor type and output directories. Make sure the directories are created before training the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "import os\n",
    "\n",
    "dataset_key='dataset/SLC6A3_Ki_curated.csv'\n",
    "descriptor_type = 'rdkit_raw'\n",
    "model_dir = 'dataset/SLC6A3_models'\n",
    "best_model_dir = 'dataset/SLC6A3_models/best_models'\n",
    "split_uuid = \"c35aeaab-910c-4dcf-8f9f-04b55179aa1a\"\n",
    "\n",
    "\n",
    "if not os.path.exists(f'./{best_model_dir}'):\n",
    "    os.mkdir(f'./{best_model_dir}')\n",
    "    \n",
    "if not os.path.exists(f'./{model_dir}'):\n",
    "    os.mkdir(f'./{model_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter dictionary settings.\n",
    "- `'hyperparam':True` This setting indicates that we are performing\n",
    "a hyperparameter search instead of just training one model.\n",
    "- `'previously_featurized':'True'` This tells AMPL to search for\n",
    "previously generated features in `../dataset/scaled_descriptors` instead\n",
    "of regenerating them on the fly.\n",
    "- `'search_type':'hyperopt'` This specifies the hyperparameter\n",
    "search method. Other options include grid, random, and geometric.\n",
    "Specifications for each hyperparameter search method is different,\n",
    "please refer to the full documentation. Here we are using the\n",
    "`Bayesian optimization` method.\n",
    "- `'model_type':'RF|10'` This means **[AMPL](https://github.com/ATOMScience-org/AMPL)** will try 10 times to \n",
    "find the best set of hyperparameters using random forests. In \n",
    "production this parameter could be set to 100 or more.\n",
    "- `'rfe':'uniformint|8,512'` The `Bayesian optimizer` will uniformly\n",
    "search between 8 and 512 for the best number of random forest estimators.\n",
    "Similarly `rfd` stands for random forest depth and `rff` stands for\n",
    "random forest features.\n",
    "- `result_dir` Now expects two parameters. The first directory\n",
    "will contain the best trained models while the second directory will\n",
    "contain all models trained in the search.\n",
    "\n",
    "Regression models are optimized using root mean squared loss and\n",
    "classification models are optimized using area under the \n",
    "receiver operating characteristic curve.\n",
    "A full list of parameters can be found on our github \n",
    "**[here](https://github.com/ATOMScience-org/AMPL/blob/master/atomsci/ddm/docs/PARAMETERS.md)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"hyperparam\": \"True\",\n",
    "    \"prediction_type\": \"regression\",\n",
    "\n",
    "    \"dataset_key\": dataset_key,\n",
    "    \"id_col\": \"compound_id\",\n",
    "    \"smiles_col\": \"base_rdkit_smiles\",\n",
    "    \"response_cols\": \"avg_pKi\",\n",
    "\n",
    "    \"splitter\":\"scaffold\",\n",
    "    \"split_uuid\": split_uuid,\n",
    "    \"previously_split\": \"True\",\n",
    "\n",
    "    \"featurizer\": \"computed_descriptors\",\n",
    "    \"descriptor_type\" : descriptor_type,\n",
    "    \"previously_featurized\": \"True\",\n",
    "    \"transformers\": \"True\",\n",
    "\n",
    "    \"search_type\": \"hyperopt\",\n",
    "    \"model_type\": \"RF|10\",\n",
    "    \"rfe\": \"uniformint|8,512\",\n",
    "    \"rfd\": \"uniformint|6,32\",\n",
    "    \"rff\": \"uniformint|8,200\",\n",
    "\n",
    "    \"result_dir\": f\"./{best_model_dir},./{model_dir}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **tutorial 4** we directly imported the `parameter_parser` and `model_pipeline` objects to parse the config dict and train a single model. Here, we use `hyperparameter_search_wrapper` to handle many models for us. First we build the search by creating a list of parameters to use, and then we run the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_performance|train_r2|train_rms|valid_r2|valid_rms|test_r2|test_rms|model_params|model\n",
      "\n",
      "rf_estimators: 306, rf_max_depth: 8, rf_max_feature: 185\n",
      "RF model with computed_descriptors and rdkit_raw      \n",
      "  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:25:06] UFFTYPER: Unrecognized charge state for atom: 10\n",
      "\n",
      "[11:25:06] UFFTYPER: Unrecognized charge state for atom: 11\n",
      "\n",
      "[11:29:57] UFFTYPER: Unrecognized charge state for atom: 6\n",
      "\n",
      "[11:30:20] UFFTYPER: Unrecognized charge state for atom: 4\n",
      "\n",
      "[11:30:20] UFFTYPER: Unrecognized charge state for atom: 4\n",
      "\n",
      "[11:30:20] UFFTYPER: Unrecognized charge state for atom: 4\n",
      "\n",
      "[11:30:36] UFFTYPER: Unrecognized charge state for atom: 1\n",
      "\n",
      "2024-04-15 11:32:15,069 Featurized file already exists. Continuing:\n",
      "2024-04-15 11:32:15,088 Previous dataset split restored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_performance|0.869|0.450|0.453|0.894|0.427|0.922|306_8_185|./dataset/SLC6A3_models/SLC6A3_Ki_curated_model_449b4803-9aeb-4be0-8704-941022a95671.tar.gz\n",
      "\n",
      "rf_estimators: 372, rf_max_depth: 24, rf_max_feature: 123                          \n",
      "RF model with computed_descriptors and rdkit_raw                                   \n",
      " 10%|█         | 1/10 [08:02<1:12:24, 482.74s/trial, best loss: 0.5472864835369445]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:33:06] UFFTYPER: Unrecognized charge state for atom: 10\n",
      "\n",
      "[11:33:06] UFFTYPER: Unrecognized charge state for atom: 11\n",
      "\n",
      "[11:38:03] UFFTYPER: Unrecognized charge state for atom: 6\n",
      "\n",
      "[11:38:24] UFFTYPER: Unrecognized charge state for atom: 4\n",
      "\n",
      "[11:38:24] UFFTYPER: Unrecognized charge state for atom: 4\n",
      "\n",
      "[11:38:24] UFFTYPER: Unrecognized charge state for atom: 4\n",
      "\n",
      "[11:38:38] UFFTYPER: Unrecognized charge state for atom: 1\n",
      "\n",
      "2024-04-15 11:39:41,837 Featurized file already exists. Continuing:\n",
      "2024-04-15 11:39:41,867 Previous dataset split restored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_performance|0.951|0.276|0.486|0.867|0.437|0.914|372_24_123|./dataset/SLC6A3_models/SLC6A3_Ki_curated_model_6fb44e08-ffd6-476b-8d42-274818154474.tar.gz\n",
      "\n",
      "rf_estimators: 22, rf_max_depth: 8, rf_max_feature: 172                            \n",
      "RF model with computed_descriptors and rdkit_raw                                   \n",
      " 20%|██        | 2/10 [15:35<1:01:58, 464.85s/trial, best loss: 0.5144174634350562]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:40:43] UFFTYPER: Unrecognized charge state for atom: 10\n",
      "\n",
      "[11:40:43] UFFTYPER: Unrecognized charge state for atom: 11\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_performance|0.000|100.000|0.000|100.000|0.000|100.000|22_8_172|./dataset/SLC6A3_models/SLC6A3_Ki_curated_model_4db9a2a1-c04e-445a-bc63-3fae88c5cd24.tar.gz\n",
      "\n",
      "rf_estimators: 397, rf_max_depth: 20, rf_max_feature: 171                          \n",
      "RF model with computed_descriptors and rdkit_raw                                 \n",
      " 30%|███       | 3/10 [18:52<39:59, 342.82s/trial, best loss: 0.5144174634350562]"
     ]
    }
   ],
   "source": [
    "import atomsci.ddm.utils.hyperparam_search_wrapper as hsw\n",
    "import importlib\n",
    "importlib.reload(hsw)\n",
    "ampl_param = hsw.parse_params(params)\n",
    "hs = hsw.build_search(ampl_param)\n",
    "hs.run_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top scoring model will be saved in `dataset/SLC6A3_models/best_models` along with a csv file\n",
    "containing regression performance for all trained models.\n",
    "\n",
    "All of the models are saved in `dataset/SLC6A3_models`. These models can be\n",
    "explored using `get_filesystem_perf_results`. A full analysis of the hyperparameter performance is explored in **tutorial 7**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data for 12 models under dataset/SLC6A3_models\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_uuid</th>\n",
       "      <th>model_parameters_dict</th>\n",
       "      <th>best_valid_r2_score</th>\n",
       "      <th>best_test_r2_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>520295f8-2ac5-45d8-9433-d3a0e96ccf0c</td>\n",
       "      <td>{\"rf_estimators\": 170, \"rf_max_depth\": 15, \"rf...</td>\n",
       "      <td>0.497099</td>\n",
       "      <td>0.438620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37a85d2a-1c0e-48aa-bcb9-f1b0a6107f29</td>\n",
       "      <td>{\"rf_estimators\": 421, \"rf_max_depth\": 16, \"rf...</td>\n",
       "      <td>0.496579</td>\n",
       "      <td>0.421240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>d3388e81-d151-420d-b55c-b10627d1c71e</td>\n",
       "      <td>{\"rf_estimators\": 500, \"rf_max_depth\": 14, \"rf...</td>\n",
       "      <td>0.489879</td>\n",
       "      <td>0.437214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8afb64d6-993e-4d8b-9072-60dcb40d2c83</td>\n",
       "      <td>{\"rf_estimators\": 500, \"rf_max_depth\": null, \"...</td>\n",
       "      <td>0.489673</td>\n",
       "      <td>0.416391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3eb607ee-cf68-4eac-bc0c-92689443a278</td>\n",
       "      <td>{\"rf_estimators\": 382, \"rf_max_depth\": 19, \"rf...</td>\n",
       "      <td>0.484862</td>\n",
       "      <td>0.433234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              model_uuid  \\\n",
       "2   520295f8-2ac5-45d8-9433-d3a0e96ccf0c   \n",
       "5   37a85d2a-1c0e-48aa-bcb9-f1b0a6107f29   \n",
       "11  d3388e81-d151-420d-b55c-b10627d1c71e   \n",
       "0   8afb64d6-993e-4d8b-9072-60dcb40d2c83   \n",
       "6   3eb607ee-cf68-4eac-bc0c-92689443a278   \n",
       "\n",
       "                                model_parameters_dict  best_valid_r2_score  \\\n",
       "2   {\"rf_estimators\": 170, \"rf_max_depth\": 15, \"rf...             0.497099   \n",
       "5   {\"rf_estimators\": 421, \"rf_max_depth\": 16, \"rf...             0.496579   \n",
       "11  {\"rf_estimators\": 500, \"rf_max_depth\": 14, \"rf...             0.489879   \n",
       "0   {\"rf_estimators\": 500, \"rf_max_depth\": null, \"...             0.489673   \n",
       "6   {\"rf_estimators\": 382, \"rf_max_depth\": 19, \"rf...             0.484862   \n",
       "\n",
       "    best_test_r2_score  \n",
       "2             0.438620  \n",
       "5             0.421240  \n",
       "11            0.437214  \n",
       "0             0.416391  \n",
       "6             0.433234  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import atomsci.ddm.pipeline.compare_models as cm\n",
    "\n",
    "result_df = cm.get_filesystem_perf_results(\n",
    "    result_dir=model_dir,\n",
    "    pred_type='regression'\n",
    ")\n",
    "\n",
    "# sort by validation r2 score to see top performing models\n",
    "result_df = result_df.sort_values(by='best_valid_r2_score', ascending=False)\n",
    "result_df[['model_uuid','model_parameters_dict','best_valid_r2_score','best_test_r2_score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Examples for other parameters\n",
    "Below are some parameters that can be used for neural networks, \n",
    "**[XGBoost](https://en.wikipedia.org/wiki/XGBoost)** models, \n",
    "fingerprint splits and **[ECFP](https://pubs.acs.org/doi/10.1021/ci100050t)** features.\n",
    "Each set of parameters can be used to replace the parameters above. \n",
    "Trying them out is left as an exercise for the reader.\n",
    "\n",
    "#### Neural Network Hyperopt Search\n",
    "- `lr` This controls the learning rate. `loguniform|-13.8,-3` means the logarithm of \n",
    "the learning rate is uniformly distributed between `-13.8` and `-3`.\n",
    "- `ls` This controls layer sizes. `3|8,512` means 3 layers with sizes ranging\n",
    "between 8 and 512 neurons. A good strategy is to start with a fewer layers \n",
    "and slowly increase the number until performance plateaus. \n",
    "- `dp` This controls dropout. `3|0,0.4` means 3 dropout layers with\n",
    "probability of zeroing a weight between 0 and 40%. This needs to match the \n",
    "number of layers specified with `ls` and should range between 0% and 50%. \n",
    "- `max_epochs` This controls how long to train each model. Training for more\n",
    "epochs increases runtime, but allows models more time to optimize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```\n",
    "params = {\n",
    "    \"hyperparam\": \"True\",\n",
    "    \"prediction_type\": \"regression\",\n",
    "\n",
    "    \"dataset_key\": dataset_key,\n",
    "    \"id_col\": \"compound_id\",\n",
    "    \"smiles_col\": \"base_rdkit_smiles\",\n",
    "    \"response_cols\": \"avg_pKi\",\n",
    "\n",
    "    \"splitter\":\"scaffold\",\n",
    "    \"split_uuid\": split_uuid,\n",
    "    \"previously_split\": \"True\",\n",
    "\n",
    "    \"featurizer\": \"computed_descriptors\",\n",
    "    \"descriptor_type\" : descriptor_type,\n",
    "    \"transformers\": \"True\",\n",
    "\n",
    "    ### Use a NN model\n",
    "    \"search_type\": \"hyperopt\",\n",
    "    \"model_type\": \"NN|10\",\n",
    "    \"lr\": \"loguniform|-13.8,-3\",\n",
    "    \"ls\": \"uniformint|3|8,512\",\n",
    "    \"dp\": \"uniform|3|0,0.4\",\n",
    "    \"max_epochs\":100\n",
    "    ###\n",
    "\n",
    "    \"result_dir\": f\"./{best_model_dir},./{model_dir}\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost\n",
    "- `xgbg` Stands for xgb_gamma and controls the minimum loss \n",
    "reduction required to make a further partition on a leaf node of the tree.\n",
    "- `xgbl` Stands for xgb_learning_rate and controls the boosting \n",
    "learning rate searching domain of XGBoost models.\n",
    "\n",
    "```\n",
    "params = {\n",
    "    \"hyperparam\": \"True\",\n",
    "    \"prediction_type\": \"regression\",\n",
    "\n",
    "    \"dataset_key\": dataset_key,\n",
    "    \"id_col\": \"compound_id\",\n",
    "    \"smiles_col\": \"base_rdkit_smiles\",\n",
    "    \"response_cols\": \"avg_pKi\",\n",
    "\n",
    "    \"splitter\":\"scaffold\",\n",
    "    \"split_uuid\": split_uuid,\n",
    "    \"previously_split\": \"True\",\n",
    "\n",
    "    \"featurizer\": \"computed_descriptors\",\n",
    "    \"descriptor_type\" : descriptor_type,\n",
    "    \"transformers\": \"True\",\n",
    "\n",
    "    ### Use an XGBoost model\n",
    "    \"search_type\": \"hyperopt\",\n",
    "    \"model_type\": \"xgboost|10\",\n",
    "    \"xgbg\": \"uniform|0,0.2\",\n",
    "    \"xgbl\": \"loguniform|-2,2\",\n",
    "    ###\n",
    "\n",
    "    \"result_dir\": f\"./{best_model_dir},./{model_dir}\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fingerprint Split\n",
    "This trains an XGBoost model using a \n",
    "fingerprint split created in **tutorial 3**.\n",
    "\n",
    "```\n",
    "fp_split_uuid=\"be60c264-6ac0-4841-a6b6-41bf846e4ae4\"\n",
    "\n",
    "params = {\n",
    "    \"hyperparam\": \"True\",\n",
    "    \"prediction_type\": \"regression\",\n",
    "\n",
    "    \"dataset_key\": dataset_key,\n",
    "    \"id_col\": \"compound_id\",\n",
    "    \"smiles_col\": \"base_rdkit_smiles\",\n",
    "    \"response_cols\": \"avg_pKi\",\n",
    "\n",
    "    ### Use a fingerprint split\n",
    "    \"splitter\":\"fingerprint\",\n",
    "    \"split_uuid\": fp_split_uuid,\n",
    "    \"previously_split\": \"True\",\n",
    "    ###\n",
    "\n",
    "    \"featurizer\": \"computed_descriptors\",\n",
    "    \"descriptor_type\" : descriptor_type,\n",
    "    \"transformers\": \"True\",\n",
    "\n",
    "    \"search_type\": \"hyperopt\",\n",
    "    \"model_type\": \"xgboost|10\",\n",
    "    \"xgbg\": \"uniform|0,0.2\",\n",
    "    \"xgbl\": \"loguniform|-2,2\",\n",
    "\n",
    "    \"result_dir\": f\"./{best_model_dir},./{model_dir}\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ECFP Features\n",
    "This uses an XGBoost model with ECFP features and a scaffold split.\n",
    "\n",
    "```\n",
    "fp_split_uuid=\"be60c264-6ac0-4841-a6b6-41bf846e4ae4\"\n",
    "\n",
    "params = {\n",
    "    \"hyperparam\": \"True\",\n",
    "    \"prediction_type\": \"regression\",\n",
    "\n",
    "    \"dataset_key\": dataset_key,\n",
    "    \"id_col\": \"compound_id\",\n",
    "    \"smiles_col\": \"base_rdkit_smiles\",\n",
    "    \"response_cols\": \"avg_pKi\",\n",
    "\n",
    "    \"splitter\":\"scaffold\",\n",
    "    \"split_uuid\": split_uuid,\n",
    "    \"previously_split\": \"True\",\n",
    "\n",
    "    ### Use ECFP Features\n",
    "    \"featurizer\": \"ecfp\",\n",
    "    \"ecfp_radius\" : 2,\n",
    "    \"ecfp_size\" : 1024,\n",
    "    \"transformers\": \"True\",\n",
    "    ###\n",
    "\n",
    "    \"search_type\": \"hyperopt\",\n",
    "    \"model_type\": \"xgboost|10\",\n",
    "    \"xgbg\": \"uniform|0,0.2\",\n",
    "    \"xgbl\": \"loguniform|-2,2\",\n",
    "\n",
    "    \"result_dir\": f\"./{best_model_dir},./{model_dir}\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **tutorial 7**, we analyze the performance of these large sets of models to select the best `hyperparameters` for `production models`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMPL_virtualenv_1.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
